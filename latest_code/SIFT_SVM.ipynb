{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"manith04\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"KGAT_3281ee8bdd9e4c418ae9f767420667f3\"\n",
        "\n",
        "import os\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Initialize API\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "# Dataset ID\n",
        "dataset = \"manith04/rt-image\"\n",
        "\n",
        "# Download directory\n",
        "download_dir = \"./rt-image\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# Download and unzip\n",
        "api.dataset_download_files(dataset, path=download_dir, unzip=True)\n",
        "\n",
        "print(f\"Dataset downloaded and extracted to {download_dir}\")\n"
      ],
      "metadata": {
        "id": "CatrcYTbr2Q5",
        "outputId": "ba1cac8c-55cd-4b2f-a011-eec866e19b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/manith04/rt-image\n",
            "Dataset downloaded and extracted to ./rt-image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSwGOB9dy4Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/rt-image/MVSA"
      ],
      "metadata": {
        "id": "MDxgaERzr2S_",
        "outputId": "4cad1a35-84f5-4020-a9d5-6a2eb7db65d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rt-image/MVSA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image, ImageFile\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# -------------------------\n",
        "# 1. Load dataset\n",
        "# -------------------------\n",
        "df = pd.read_csv(\"mvsa_image_soft_labels.txt\", sep=\"\\t\")  # adjust sep if needed\n",
        "df['ID'] = df['ID'].astype(int).astype(str)\n",
        "\n",
        "# Stratified split\n",
        "df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42, stratify=df.iloc[:,1])\n",
        "df_val, df_test  = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp.iloc[:,1])\n",
        "\n",
        "print(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
        "\n",
        "# -------------------------\n",
        "# 2. Dataset class (robust)\n",
        "# -------------------------\n",
        "class SafeImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_map = {'negative':0, 'neutral':1, 'positive':2}\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.data.iloc[idx]\n",
        "            img_id = str(row['ID'])\n",
        "            img_path = None\n",
        "            for ext in [\"png\", \"jpg\", \"jpeg\"]:\n",
        "                temp_path = os.path.join(self.image_dir, f\"{img_id}.{ext}\")\n",
        "                if os.path.exists(temp_path):\n",
        "                    img_path = temp_path\n",
        "                    break\n",
        "\n",
        "            if img_path is None:\n",
        "                raise FileNotFoundError\n",
        "\n",
        "            image = Image.open(img_path).convert(\"L\")  # grayscale for SIFT\n",
        "            image = np.array(image)\n",
        "\n",
        "            label_cell = row.iloc[1]\n",
        "            label_str = str(label_cell)\n",
        "            if ',' not in label_str:\n",
        "                image_label = 'neutral'\n",
        "            else:\n",
        "                _, image_label = label_str.split(',')\n",
        "\n",
        "            label = self.label_map.get(image_label, 1)\n",
        "\n",
        "        except Exception:\n",
        "            # On error, return blank image and neutral label\n",
        "            image = np.zeros((456,456), dtype=np.uint8)\n",
        "            label = 1\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# -------------------------\n",
        "# 3. Load image IDs and labels (no full images in memory)\n",
        "# -------------------------\n",
        "train_dataset = SafeImageDataset(df_train, \"data\")\n",
        "val_dataset   = SafeImageDataset(df_val, \"data\")\n",
        "test_dataset  = SafeImageDataset(df_test, \"data\")\n",
        "\n",
        "# -------------------------\n",
        "# 4. SIFT + MiniBatchKMeans (fit in batches)\n",
        "# -------------------------\n",
        "sift = cv2.SIFT_create()\n",
        "VOCAB_SIZE = 200\n",
        "kmeans = MiniBatchKMeans(n_clusters=VOCAB_SIZE, batch_size=100, verbose=1, random_state=42)\n",
        "\n",
        "# Batch-wise KMeans fitting\n",
        "BATCH_SIZE = 50\n",
        "print(\"Fitting KMeans in batches...\")\n",
        "for start in range(0, len(train_dataset), BATCH_SIZE):\n",
        "    batch_descs = []\n",
        "    for i in range(start, min(start+BATCH_SIZE, len(train_dataset))):\n",
        "        img, _ = train_dataset[i]\n",
        "        kp, desc = sift.detectAndCompute(img, None)\n",
        "        if desc is not None:\n",
        "            batch_descs.append(desc)\n",
        "    if batch_descs:\n",
        "        batch_descs = np.vstack(batch_descs)\n",
        "        kmeans.partial_fit(batch_descs)\n",
        "\n",
        "# -------------------------\n",
        "# 5. Function to compute BoVW histograms batch-wise\n",
        "# -------------------------\n",
        "def compute_histograms(dataset, batch_size=50):\n",
        "    features, labels = [], []\n",
        "    for start in range(0, len(dataset), batch_size):\n",
        "        for i in range(start, min(start+batch_size, len(dataset))):\n",
        "            img, lbl = dataset[i]\n",
        "            kp, desc = sift.detectAndCompute(img, None)\n",
        "            if desc is None:\n",
        "                desc = np.zeros((1,128))\n",
        "            words = kmeans.predict(desc)\n",
        "            hist, _ = np.histogram(words, bins=np.arange(VOCAB_SIZE+1))\n",
        "            features.append(hist)\n",
        "            labels.append(lbl)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# -------------------------\n",
        "# 6. Compute BoVW histograms\n",
        "# -------------------------\n",
        "print(\"Computing BoVW histograms for train set...\")\n",
        "X_train, y_train = compute_histograms(train_dataset)\n",
        "print(\"Computing BoVW histograms for val set...\")\n",
        "X_val, y_val     = compute_histograms(val_dataset)\n",
        "print(\"Computing BoVW histograms for test set...\")\n",
        "X_test, y_test   = compute_histograms(test_dataset)\n",
        "\n",
        "# -------------------------\n",
        "# 7. Normalize\n",
        "# -------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# -------------------------\n",
        "# 8. Train SVM\n",
        "# -------------------------\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------\n",
        "# 9. Evaluate\n",
        "# -------------------------\n",
        "y_val_pred = svm.predict(X_val)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "y_test_pred = svm.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akxRblk10-OY",
        "outputId": "3f34b0ba-e7e4-4ad5-e09f-48bad600d23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 15680, Val: 1960, Test: 1960\n",
            "Fitting KMeans in batches...\n"
          ]
        }
      ]
    }
  ]
}