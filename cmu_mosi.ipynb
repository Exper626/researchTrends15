{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776414d-e114-405b-82b4-16467baf6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmsdk import mmdatasdk\n",
    "import os\n",
    "\n",
    "data_path = r\"dataset\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "features = {\n",
    "    \"covarep\": data_path + \"/CMU_MOSI_COVAREP.csd\",\n",
    "    \"facet41\": data_path + \"/CMU_MOSI_Visual_Facet_41.csd\",\n",
    "    \"facet42\": data_path + \"/CMU_MOSI_Visual_Facet_42.csd\",\n",
    "    \"opensmile_eb10\": data_path + \"/CMU_MOSI_OpenSmile_EB10.csd\",\n",
    "    \"words\": data_path + \"/CMU_MOSI_TimestampedWords.csd\",\n",
    "    \"wordvec\": data_path + \"/CMU_MOSI_TimestampedWordVectors.csd\",\n",
    "    \"labels\": data_path + \"/CMU_MOSI_Opinion_Labels.csd\"\n",
    "}\n",
    "dataset = mmdatasdk.mmdataset(features)\n",
    "dataset.align(\"labels\")\n",
    "print(dataset[\"covarep\"].keys())\n",
    "\n",
    "sample = list(dataset[\"covarep\"].keys())[0]\n",
    "print(dataset[\"covarep\"][sample]['features'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694597e-c12c-4959-b87d-30cd5b35a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "audio_tensors = [torch.tensor(x, dtype=torch.float32) for x in X_audio]\n",
    "audio_padded = pad_sequence(audio_tensors, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55cca80c-cbbc-4b0c-a5f5-f258c64eb071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03bSnISJMiM[0]\n",
      "2183\n",
      "audio_feat = tensor([[2.6000e+02, 1.0000e+00, 2.1585e-01,  ..., 4.3386e-01, 4.0485e-01,\n",
      "         2.5323e-01],\n",
      "        [2.5900e+02, 1.0000e+00, 1.0248e-01,  ..., 4.2773e-01, 4.1049e-01,\n",
      "         2.4466e-01],\n",
      "        [2.6300e+02, 1.0000e+00, 2.6963e-01,  ..., 4.1255e-01, 4.0917e-01,\n",
      "         2.3894e-01],\n",
      "        ...,\n",
      "        [2.0550e+02, 1.0000e+00, 1.7049e-01,  ..., 6.4619e-02, 1.5919e-01,\n",
      "         2.2397e-01],\n",
      "        [2.0650e+02, 1.0000e+00, 2.2382e-01,  ..., 4.7717e-02, 1.7221e-01,\n",
      "         2.4578e-01],\n",
      "        [2.0700e+02, 1.0000e+00, 1.8770e-01,  ..., 3.5531e-02, 1.8578e-01,\n",
      "         2.6337e-01]])\n",
      "video_feat = tensor([[ 1.3100e+02,  4.1000e+01,  2.7700e+02,  ...,  2.2776e-01,\n",
      "          1.1950e-01, -6.5578e-01],\n",
      "        [ 1.3200e+02,  4.1000e+01,  2.7600e+02,  ...,  2.6919e-01,\n",
      "          2.4537e-01, -5.9025e-01],\n",
      "        [ 1.4000e+02,  4.1000e+01,  2.7900e+02,  ...,  3.6260e-01,\n",
      "          4.3830e-01, -5.2462e-01],\n",
      "        ...,\n",
      "        [ 2.9100e+02,  6.7000e+01,  2.7000e+02,  ...,  5.1788e-01,\n",
      "          3.8348e-01, -8.7795e-01],\n",
      "        [ 2.9000e+02,  6.8000e+01,  2.7000e+02,  ...,  4.8906e-01,\n",
      "          4.2954e-01, -7.7648e-01],\n",
      "        [ 2.8800e+02,  6.5000e+01,  2.6600e+02,  ...,  4.4741e-01,\n",
      "          4.5585e-01, -6.8648e-01]])\n",
      "text_feat = tensor([[-0.1281,  0.0217, -0.3318,  ...,  0.0722, -0.3984,  0.1006],\n",
      "        [-0.1691, -0.2640, -0.2258,  ..., -0.3505, -0.2592,  0.2898],\n",
      "        [-0.1281,  0.0217, -0.3318,  ...,  0.0722, -0.3984,  0.1006],\n",
      "        ...,\n",
      "        [-0.1281,  0.0217, -0.3318,  ...,  0.0722, -0.3984,  0.1006],\n",
      "        [-0.2116,  0.5088, -0.3581,  ..., -0.1807,  0.2812,  0.1698],\n",
      "        [-0.4263,  0.4431, -0.3452,  ..., -0.4303, -0.0689,  0.1287]])\n",
      "label = tensor([[2.4000]])\n",
      "Predicted sentiment: -0.02041596733033657\n",
      "True label: 2.4000000953674316\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "sample = list(dataset[\"labels\"].keys())[0]\n",
    "print(sample)\n",
    "\n",
    "print(len(dataset[\"labels\"].keys()))\n",
    "\n",
    "# Audio features\n",
    "audio_feat = torch.tensor(dataset[\"covarep\"][sample]['features'], dtype=torch.float32)  # (T_audio, 74)\n",
    "print(f\"{audio_feat = }\")\n",
    "\n",
    "\n",
    "# Video features\n",
    "video_feat = torch.tensor(dataset[\"facet41\"][sample]['features'], dtype=torch.float32)  # (T_video, 41)\n",
    "print(f\"{video_feat = }\")\n",
    "      \n",
    "# Text features\n",
    "text_feat = torch.tensor(dataset[\"wordvec\"][sample]['features'], dtype=torch.float32)  # (T_text, 300)\n",
    "print(f\"{text_feat = }\")\n",
    "\n",
    "      \n",
    "# Segment label\n",
    "label = torch.tensor(dataset[\"labels\"][sample]['features'], dtype=torch.float32)  # (1,)\n",
    "print(f\"{label = }\")\n",
    "\n",
    "\n",
    "audio_lstm = nn.LSTM(input_size=74, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "video_lstm = nn.LSTM(input_size=47, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "text_lstm  = nn.LSTM(input_size=300, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "\n",
    "# Add batch dimension (batch=1)\n",
    "audio_in = audio_feat.unsqueeze(0)  # (1, T_audio, 74)\n",
    "video_in = video_feat.unsqueeze(0)  # (1, T_video, 41)\n",
    "text_in  = text_feat.unsqueeze(0)   # (1, T_text, 300)\n",
    "\n",
    "# Forward pass through LSTM\n",
    "audio_out, _ = audio_lstm(audio_in)  # (1, T_audio, 256)\n",
    "video_out, _ = video_lstm(video_in)  # (1, T_video, 256)\n",
    "text_out,  _ = text_lstm(text_in)    # (1, T_text, 256)\n",
    "\n",
    "\n",
    "\n",
    "def mean_pool(x):\n",
    "    # x: (1, T, D)\n",
    "    return x.mean(dim=1)  # (1, D)\n",
    "\n",
    "audio_vec = mean_pool(audio_out)  # (1, 256)\n",
    "video_vec = mean_pool(video_out)  # (1, 256)\n",
    "text_vec  = mean_pool(text_out)   # (1, 256)\n",
    "\n",
    "\n",
    "fused = torch.cat([audio_vec, video_vec, text_vec], dim=1)  # (1, 768)\n",
    "\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(256*3, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    \n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    \n",
    "    nn.Linear(128, 1)  # predict sentiment\n",
    ")\n",
    "\n",
    "y_pred = mlp(fused)\n",
    "print(\"Predicted sentiment:\", y_pred.item())\n",
    "print(\"True label:\", label.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "352cca94-e998-4b63-81a8-904a2daebcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique video count: 92\n",
      "Video IDs: {'HEsqda8_d0Q', 'Njd1F0vZSm4', 'cW1FSBF59ik', 'cXypl4FnoZo', 'VbQk4H8hgr0', '73jzhE8R1TQ', 'G6GlGvlkxAQ', 'QN9ZIUWUXsY', 'rnaNMUZpvvg', 'dq3Nf_lMPnE', 'pLTX3ipuDJI', 'Qr1Ca94K55A', 'jUzDDGyPkXU', 'PZ-lDQFboO8', 'v0zCBqDeKcE', 'fvVhgmXxadc', 'vyB00TXsimI', 'tmZoasNr4rU', 'OtBXNcAL_lE', 'WKA5OygbEKI', 'BI97DNYfe5I', 'nbWiPyCm4g0', 'zhpQhgha_KU', '03bSnISJMiM', '7JsX8y1ysxY', '6_0THN4chvY', 'k5Y_838nuGo', 'IumbAb8q2dM', 'yDtzw_Y-7RU', 'G-xst2euQUc', '6Egk_28TtTM', 'POKffnXeBds', 'ob23OKe5a9Q', '5W7Z1C_fDaE', 'VCslbP0mgZI', 'OQvJTdtJ2H4', 'I5y0__X72p0', 'tIrG4oNLFzE', '8d-gEyoeBzc', 'atnd_PF-Lbs', 'cM3Yna7AavY', 'yvsjCA6Y5Fc', 'MLal-t_vJPM', 'nzpVDcQ0ywM', 'Clx4VXItLTE', 'bOL9jKpeJRs', 'vvZ4IcEtiZc', 'ZUXBRvtny7o', 'Iu2PFX3z_1s', '0h-zjBukYpk', 'Nzq88NnDkEk', 'iiK8YX8oH1E', 'Dg_0XKD0Mf4', '8OtFthrtaJM', 'tStelxIAHjw', 'aiEXnCPZubE', 'Vj1wYRQjB-o', '_dI--eQ6qVU', '9J25DZhivz8', '2iD-tVS8NPw', 'TvyZBvOMOTc', 'BvYR0L6f2Ig', 'd3_k5Xpfmik', 'Af8D0E4ZXaw', 'W8NXH0Djyww', 'Sqr0AcuoNnk', '9c67fiY0wGQ', '9qR7uwkblbs', '1DmNV9C1hbY', 'f9O3YtZ2VfI', '2WGyTLYerpo', 'Oz06ZWiO20M', '1iG0909rllw', 'BXuRRbG0Ugk', 'X3j2zQgwYgE', 'c7UH_rxdZv4', 'BioHAh1qJAQ', 'Bfr499ggo-0', 'Ci-AH39fi3Y', 'bvLlb-M3UXU', 'd6hH302o4v8', 'lXPQBPVc5Cw', 'GWuJjcEuzt8', 'phBUpBr1hSo', '8qrpnFRGt2A', 'wMbj6ajWbic', 'Jkswaaud0hk', 'etzxEpPuc6I', 'LSi-o-IrDMs', 'f_pcplsH_V0', 'ZAIRrfG22O0', '9T9Hf74oK10'}\n"
     ]
    }
   ],
   "source": [
    "segments = dataset[\"labels\"].keys()\n",
    "\n",
    "video_ids = set()\n",
    "\n",
    "for seg in segments:\n",
    "    vid = seg.split(\"[\")[0]   # extract part before \"[\"\n",
    "    video_ids.add(vid)\n",
    "\n",
    "print(\"Unique video count:\", len(video_ids))\n",
    "print(\"Video IDs:\", video_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
