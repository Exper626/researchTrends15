{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!uv pip install evaluate -q\n\nimport tensorflow_datasets as tfds\nimport torch\nimport numpy as np\nfrom datasets import Dataset, Audio\nfrom transformers import (\n    Wav2Vec2Processor, \n    Wav2Vec2ForSequenceClassification, \n    TrainingArguments, \n    Trainer\n)\nimport evaluate\nimport IPython.display as ipd\n\n\n\nimport torch\nimport numpy as np\nimport evaluate\nfrom datasets import load_from_disk\nfrom transformers import (\n    TrainingArguments,\n    Trainer,\n    Wav2Vec2Processor, \n    Wav2Vec2ForSequenceClassification,\n    Wav2Vec2FeatureExtractor\n)\n\nimport os\nfrom transformers.trainer_utils import get_last_checkpoint\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:05:21.808781Z","iopub.execute_input":"2025-12-15T04:05:21.809479Z","iopub.status.idle":"2025-12-15T04:05:22.127341Z","shell.execute_reply.started":"2025-12-15T04:05:21.809446Z","shell.execute_reply":"2025-12-15T04:05:22.126272Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"ds_data, ds_info = tfds.load(\n    \"crema_d\",\n    with_info=True,\n    as_supervised=False,\n    split=['train', 'validation', 'test']\n)\n\n\n\n# Extract label mappings (0=Neutral, 1=Happy, etc.) from TFDS metadata\nlabel_names = ds_info.features['label'].names\nlabel2id = {label: i for i, label in enumerate(label_names)}\nid2label = {i: label for i, label in enumerate(label_names)}\nprint(f\"Labels found: {label_names}\")\n\n# --- 2. Bridge: Convert TFDS to Hugging Face Dataset ---\n# Wav2Vec2 Trainer works best with Hugging Face Datasets. \n# Since CREMA-D is small (~2GB), we can convert it in memory.\n\ndef tfds_to_hf_dataset(tf_dataset):\n    data_dict = {\"audio\": [], \"label\": []}\n    for sample in tf_dataset:\n        audio = sample['audio'].numpy()\n        label = sample['label'].numpy()\n        \n        # Normalize audio if it's integer PCM (Wav2Vec2 expects float inputs)\n        # CREMA-D in TFDS is often int64; we convert to float32\n        audio = audio.astype(np.float32)\n        if np.abs(audio).max() > 1.0:\n            audio = audio / 32768.0  # Normalize 16-bit PCM to [-1, 1]\n            \n        data_dict[\"audio\"].append(audio)\n        data_dict[\"label\"].append(label)\n    \n    return Dataset.from_dict(data_dict)\n\nprint(\"Converting TFDS to Hugging Face format... (this may take a minute)\")\ntrain_dataset = tfds_to_hf_dataset(ds_data[0])\neval_dataset = tfds_to_hf_dataset(ds_data[1])\ntest_dataset = tfds_to_hf_dataset(ds_data[2])\n\n\nprint(\"Saving converted dataset to disk...\")\ntrain_dataset.save_to_disk(\"./crema_hf/train\")\neval_dataset.save_to_disk(\"./crema_hf/eval\")\ntest_dataset.save_to_disk(\"./crema_hf/test\")\n\nprint(\"Saved! You can now load it later using from_disk()\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T03:46:47.793904Z","iopub.execute_input":"2025-12-15T03:46:47.794800Z","iopub.status.idle":"2025-12-15T03:47:03.992407Z","shell.execute_reply.started":"2025-12-15T03:46:47.794767Z","shell.execute_reply":"2025-12-15T03:47:03.991664Z"}},"outputs":[{"name":"stdout","text":"Labels found: ['NEU', 'HAP', 'SAD', 'ANG', 'FEA', 'DIS']\nConverting TFDS to Hugging Face format... (this may take a minute)\nSaving converted dataset to disk...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/5144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"457e3d6829d5433699fdac172ebcef6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/738 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ab97633d6904cb4bcc6a88fe3bc23aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1556 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0428a47d7748bd8907ceeea175ffef"}},"metadata":{}},{"name":"stdout","text":"Saved! You can now load it later using from_disk()\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"Loading dataset from disk...\")\ntrain_dataset = load_from_disk(\"./crema_hf/train\")\neval_dataset  = load_from_disk(\"./crema_hf/eval\")\ntest_dataset  = load_from_disk(\"./crema_hf/test\")\n\n\nlabel_names = [\"NEU\", \"HAP\", \"SAD\", \"ANG\", \"FEA\", \"DIS\"]\nlabel2id = {label: i for i, label in enumerate(label_names)}\nid2label = {i: label for i, label in enumerate(label_names)}\n\nprint(f\"Labels configured: {label_names}\")\n\n\nmodel_id = \"facebook/hubert-base-ls960\"\n# model_id = \"facebook/wav2vec2-base\"\n# model_id = \"microsoft/wavlm-base\"\n\nprocessor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)\ntarget_sampling_rate = processor.sampling_rate\n\n\ndef preprocess_function(examples):\n    audio_arrays = examples[\"audio\"]\n    return processor(\n        audio_arrays,\n        sampling_rate=target_sampling_rate,\n        max_length=target_sampling_rate * 5,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n\nprint(\"Preprocessing datasets...\")\nencoded_train = train_dataset.map(preprocess_function, batched=True)\nencoded_eval  = eval_dataset.map(preprocess_function, batched=True)\nencoded_test  = test_dataset.map(preprocess_function, batched=True)\n\n\nencoded_train = encoded_train.rename_column(\"label\", \"labels\")\nencoded_eval  = encoded_eval.rename_column(\"label\", \"labels\")\nencoded_test  = encoded_test.rename_column(\"label\", \"labels\")\n\ncolumns = [\"input_values\", \"labels\"]\nencoded_train.set_format(\"torch\", columns=columns)\nencoded_eval.set_format(\"torch\", columns=columns)\nencoded_test.set_format(\"torch\", columns=columns)\n\n\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    model_id,\n    num_labels=len(label_names),\n    label2id=label2id,\n    id2label=id2label\n)\n\nmodel.freeze_feature_extractor()\n\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    preds = np.argmax(eval_pred.predictions, axis=1)\n    return accuracy.compute(predictions=preds, references=eval_pred.label_ids)\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./hubert-base-emotion-model\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=20,\n    learning_rate=3e-5,\n    fp16=True,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_eval,\n    tokenizer=processor,  \n    compute_metrics=compute_metrics,\n)\n\n\nif torch.cuda.is_available():\n    print(f\"Training on GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: GPU not detected.\")\n\n\noutput_dir = \"./hubert-base-emotion-model\"\n\nif os.path.isdir(output_dir) and get_last_checkpoint(output_dir) is not None:\n    last_checkpoint = get_last_checkpoint(output_dir)\n    print(f\"Resuming training from {last_checkpoint}\")\n    trainer.train(resume_from_checkpoint=last_checkpoint)\n\nelse:\n    print(\"No checkpoint found, training from scratch\")\n    trainer.train()\n\n\nprint(\"Evaluating on test set...\")\ntrainer.save_model(\"./hubert-base-emotion-model/final_model\")\nprocessor.save_pretrained(\"./hubert-base-emotion-model/final_processor\")\n\n\ntest_results = trainer.predict(encoded_test)\nprint(f\"Test Accuracy: {test_results.metrics['test_accuracy']:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:05:24.719135Z","iopub.execute_input":"2025-12-15T04:05:24.719951Z","iopub.status.idle":"2025-12-15T04:59:27.426468Z","shell.execute_reply.started":"2025-12-15T04:05:24.719905Z","shell.execute_reply":"2025-12-15T04:59:27.425306Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from disk...\nLabels configured: ['NEU', 'HAP', 'SAD', 'ANG', 'FEA', 'DIS']\nPreprocessing datasets...\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type hubert to instantiate a model of type wav2vec2. This is not supported for all configurations of models and can yield errors.\nSome weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training on GPU: Tesla T4\nNo checkpoint found, training from scratch\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4831' max='6440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4831/6440 53:45 < 17:54, 1.50 it/s, Epoch 15/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.466900</td>\n      <td>1.308947</td>\n      <td>0.501355</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.104800</td>\n      <td>1.017069</td>\n      <td>0.655827</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.914300</td>\n      <td>0.956272</td>\n      <td>0.670732</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.814800</td>\n      <td>0.937967</td>\n      <td>0.681572</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.721200</td>\n      <td>0.829177</td>\n      <td>0.739837</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.679100</td>\n      <td>0.965837</td>\n      <td>0.704607</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.602900</td>\n      <td>0.816032</td>\n      <td>0.750678</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.459100</td>\n      <td>1.019663</td>\n      <td>0.722222</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.352200</td>\n      <td>0.945358</td>\n      <td>0.754743</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.450800</td>\n      <td>0.927353</td>\n      <td>0.757453</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.382600</td>\n      <td>0.879049</td>\n      <td>0.765583</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.446500</td>\n      <td>0.971640</td>\n      <td>0.761518</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.319500</td>\n      <td>1.015625</td>\n      <td>0.757453</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.327900</td>\n      <td>1.085687</td>\n      <td>0.752033</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.264200</td>\n      <td>0.926851</td>\n      <td>0.777778</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/190: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/113676090.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No checkpoint found, training from scratch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2658\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 332730048 vs 332729936"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:626] . unexpected pos 332730048 vs 332729936","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"!ls ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:04:29.650241Z","iopub.execute_input":"2025-12-15T04:04:29.650563Z","iopub.status.idle":"2025-12-15T04:04:29.850512Z","shell.execute_reply.started":"2025-12-15T04:04:29.650536Z","shell.execute_reply":"2025-12-15T04:04:29.849541Z"}},"outputs":[{"name":"stdout","text":"crema_hf  hubert-base-emotion-model\n","output_type":"stream"}],"execution_count":22}]}