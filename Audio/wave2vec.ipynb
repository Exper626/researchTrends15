{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!uv pip install evaluate -q\n\nimport tensorflow_datasets as tfds\nimport torch\nimport numpy as np\nfrom datasets import Dataset, Audio\nfrom transformers import (\n    Wav2Vec2Processor, \n    Wav2Vec2ForSequenceClassification, \n    TrainingArguments, \n    Trainer\n)\nimport evaluate\nimport IPython.display as ipd\n\n\n\nimport torch\nimport numpy as np\nimport evaluate\nfrom datasets import load_from_disk\nfrom transformers import (\n    TrainingArguments,\n    Trainer,\n    Wav2Vec2Processor, \n    Wav2Vec2ForSequenceClassification,\n    Wav2Vec2FeatureExtractor\n)\n\n\nimport os\nfrom transformers.trainer_utils import get_last_checkpoint\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:21:35.363183Z","iopub.execute_input":"2025-12-15T04:21:35.363819Z","iopub.status.idle":"2025-12-15T04:21:35.693226Z","shell.execute_reply.started":"2025-12-15T04:21:35.363793Z","shell.execute_reply":"2025-12-15T04:21:35.692039Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"ds_data, ds_info = tfds.load(\n    \"crema_d\",\n    with_info=True,\n    as_supervised=False,\n    split=['train', 'validation', 'test']\n)\n\n\n\n# Extract label mappings (0=Neutral, 1=Happy, etc.) from TFDS metadata\nlabel_names = ds_info.features['label'].names\nlabel2id = {label: i for i, label in enumerate(label_names)}\nid2label = {i: label for i, label in enumerate(label_names)}\nprint(f\"Labels found: {label_names}\")\n\n# --- 2. Bridge: Convert TFDS to Hugging Face Dataset ---\n# Wav2Vec2 Trainer works best with Hugging Face Datasets. \n# Since CREMA-D is small (~2GB), we can convert it in memory.\n\ndef tfds_to_hf_dataset(tf_dataset):\n    data_dict = {\"audio\": [], \"label\": []}\n    for sample in tf_dataset:\n        audio = sample['audio'].numpy()\n        label = sample['label'].numpy()\n        \n        # Normalize audio if it's integer PCM (Wav2Vec2 expects float inputs)\n        # CREMA-D in TFDS is often int64; we convert to float32\n        audio = audio.astype(np.float32)\n        if np.abs(audio).max() > 1.0:\n            audio = audio / 32768.0  # Normalize 16-bit PCM to [-1, 1]\n            \n        data_dict[\"audio\"].append(audio)\n        data_dict[\"label\"].append(label)\n    \n    return Dataset.from_dict(data_dict)\n\nprint(\"Converting TFDS to Hugging Face format... (this may take a minute)\")\ntrain_dataset = tfds_to_hf_dataset(ds_data[0])\neval_dataset = tfds_to_hf_dataset(ds_data[1])\ntest_dataset = tfds_to_hf_dataset(ds_data[2])\n\n\nprint(\"Saving converted dataset to disk...\")\ntrain_dataset.save_to_disk(\"./crema_hf/train\")\neval_dataset.save_to_disk(\"./crema_hf/eval\")\ntest_dataset.save_to_disk(\"./crema_hf/test\")\n\nprint(\"Saved! You can now load it later using from_disk()\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:21:38.854386Z","iopub.execute_input":"2025-12-15T04:21:38.854740Z","iopub.status.idle":"2025-12-15T04:21:56.036753Z","shell.execute_reply.started":"2025-12-15T04:21:38.854708Z","shell.execute_reply":"2025-12-15T04:21:56.035995Z"}},"outputs":[{"name":"stdout","text":"Labels found: ['NEU', 'HAP', 'SAD', 'ANG', 'FEA', 'DIS']\nConverting TFDS to Hugging Face format... (this may take a minute)\nSaving converted dataset to disk...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/5144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f7f1fd90f24fef8b91181c92c44149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/738 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6eeca7b3bc9419999185cd2ca600c17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1556 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e450edadae334f458f398c3e8f4da5ff"}},"metadata":{}},{"name":"stdout","text":"Saved! You can now load it later using from_disk()\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# -----------------------------\n# 1️⃣ Load datasets\n# -----------------------------\nprint(\"Loading dataset from disk...\")\ntrain_dataset = load_from_disk(\"./crema_hf/train\")\neval_dataset  = load_from_disk(\"./crema_hf/eval\")\ntest_dataset  = load_from_disk(\"./crema_hf/test\")\n\n# -----------------------------\n# 2️⃣ Label configuration\n# -----------------------------\nlabel_names = [\"NEU\", \"HAP\", \"SAD\", \"ANG\", \"FEA\", \"DIS\"]\nlabel2id = {label: i for i, label in enumerate(label_names)}\nid2label = {i: label for i, label in enumerate(label_names)}\nprint(f\"Labels configured: {label_names}\")\n\n# -----------------------------\n# 3️⃣ Processor & Model\n# -----------------------------\nmodel_id = \"facebook/wav2vec2-base\"\nprocessor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)\ntarget_sampling_rate = processor.sampling_rate\n\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    model_id,\n    num_labels=len(label_names),\n    label2id=label2id,\n    id2label=id2label\n)\nmodel.freeze_feature_extractor()  # freeze CNN feature extractor\n\n# -----------------------------\n# 4️⃣ Preprocessing\n# -----------------------------\ndef preprocess_function(examples):\n    audio_arrays = examples[\"audio\"]\n    return processor(\n        audio_arrays,\n        sampling_rate=target_sampling_rate,\n        max_length=target_sampling_rate * 5,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n\nprint(\"Preprocessing datasets...\")\n\nencoded_train = train_dataset.map(preprocess_function, batched=True)\nencoded_eval  = eval_dataset.map(preprocess_function, batched=True)\nencoded_test  = test_dataset.map(preprocess_function, batched=True)\n\n# Rename label column properly (assign back)\nencoded_train = encoded_train.rename_column(\"label\", \"labels\")\nencoded_eval  = encoded_eval.rename_column(\"label\", \"labels\")\nencoded_test  = encoded_test.rename_column(\"label\", \"labels\")\n\n# Set PyTorch format (in-place)\ncolumns = [\"input_values\", \"labels\"]\nencoded_train.set_format(type=\"torch\", columns=columns)\nencoded_eval.set_format(type=\"torch\", columns=columns)\nencoded_test.set_format(type=\"torch\", columns=columns)\n\n\naccuracy = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    preds = np.argmax(eval_pred.predictions, axis=1)\n    return accuracy.compute(predictions=preds, references=eval_pred.label_ids)\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2-base-emotion-model\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=20,\n    learning_rate=3e-5,\n    fp16=True,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_eval,\n    tokenizer=processor,  # feature extractor acts as tokenizer\n    compute_metrics=compute_metrics,\n)\n\n\n\nif torch.cuda.is_available():\n    print(f\"Training on GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: GPU not detected.\")\n\n\n\noutput_dir = \"./wav2vec2-base-emotion-model\"\nlast_checkpoint = get_last_checkpoint(output_dir)\n\nif last_checkpoint is not None:\n    print(f\"Resuming training from {last_checkpoint}\")\n    trainer.train(resume_from_checkpoint=last_checkpoint)\nelse:\n    print(\"No checkpoint found, training from scratch\")\n    trainer.train()\n\n\ntrainer.save_model(os.path.join(output_dir, \"final_model\"))\nprocessor.save_pretrained(os.path.join(output_dir, \"final_processor\"))\n\n\nprint(\"Evaluating on test set...\")\ntest_results = trainer.predict(encoded_test)\nprint(f\"Test Accuracy: {test_results.metrics['test_accuracy']:.4f}\")\n\n\n\nwith open(os.path.join(output_dir, \"test_metrics.json\"), \"w\") as f:\n    json.dump(test_results.metrics, f, indent=4)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:40:09.298102Z","iopub.execute_input":"2025-12-15T04:40:09.298779Z","iopub.status.idle":"2025-12-15T07:46:36.190241Z","shell.execute_reply.started":"2025-12-15T04:40:09.298751Z","shell.execute_reply":"2025-12-15T07:46:36.188793Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from disk...\nLabels configured: ['NEU', 'HAP', 'SAD', 'ANG', 'FEA', 'DIS']\n","output_type":"stream"},{"name":"stderr","text":"Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Preprocessing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57477c4f835c430580c4165962c198ab"}},"metadata":{}},{"name":"stdout","text":"Training on GPU: Tesla T4\nNo checkpoint found, training from scratch\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3865' max='6440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3865/6440 3:06:06 < 2:04:03, 0.35 it/s, Epoch 12/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.297300</td>\n      <td>1.105924</td>\n      <td>0.630081</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.940300</td>\n      <td>0.885753</td>\n      <td>0.689702</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.708500</td>\n      <td>0.792240</td>\n      <td>0.726287</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.527100</td>\n      <td>0.756590</td>\n      <td>0.741192</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.541500</td>\n      <td>0.871263</td>\n      <td>0.727642</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.467800</td>\n      <td>0.794341</td>\n      <td>0.761518</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.452100</td>\n      <td>0.866151</td>\n      <td>0.760163</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.245300</td>\n      <td>1.092163</td>\n      <td>0.726287</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.245000</td>\n      <td>0.891721</td>\n      <td>0.773713</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.206100</td>\n      <td>1.031553</td>\n      <td>0.761518</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.291700</td>\n      <td>1.337606</td>\n      <td>0.773713</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.210100</td>\n      <td>1.700090</td>\n      <td>0.773713</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/11: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4252490603.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No checkpoint found, training from scratch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2658\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 49603136 vs 49603024"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:626] . unexpected pos 49603136 vs 49603024","output_type":"error"}],"execution_count":12}]}