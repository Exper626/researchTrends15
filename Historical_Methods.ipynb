{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjfIHy7R6LeE"
   },
   "source": [
    "# **IMDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BSxbernBaAkV",
    "outputId": "e18c74dd-7fd7-49f1-ea76-852e37e436cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 112 stored elements and shape (1, 4898)>\n",
      "  Coords\tValues\n",
      "  (0, 4081)\t0.018161574791954474\n",
      "  (0, 4372)\t0.05989429002682193\n",
      "  (0, 1688)\t0.17260869183197405\n",
      "  (0, 4714)\t0.2868937029805583\n",
      "  (0, 2393)\t0.13532479702216973\n",
      "  (0, 578)\t0.2283618610749408\n",
      "  (0, 703)\t0.08695901899640085\n",
      "  (0, 2586)\t0.09754452672825938\n",
      "  (0, 3746)\t0.09391048358927476\n",
      "  (0, 4131)\t0.07964733966197848\n",
      "  (0, 1247)\t0.0721956312972466\n",
      "  (0, 1520)\t0.12936111313919155\n",
      "  (0, 3496)\t0.07943724066926205\n",
      "  (0, 4208)\t0.11654538443742532\n",
      "  (0, 4346)\t0.2746885888265036\n",
      "  (0, 3135)\t0.05562279686636129\n",
      "  (0, 4361)\t0.13333473515845054\n",
      "  (0, 3238)\t0.12596764917385855\n",
      "  (0, 223)\t0.16901333435097304\n",
      "  (0, 4888)\t0.11680196561648677\n",
      "  (0, 999)\t0.04418418996113909\n",
      "  (0, 2196)\t0.08328246387682232\n",
      "  (0, 444)\t0.0945671688358273\n",
      "  (0, 4358)\t0.06679152646353043\n",
      "  (0, 3649)\t0.08132183922770295\n",
      "  :\t:\n",
      "  (0, 3084)\t0.03372306878332425\n",
      "  (0, 2567)\t0.08853344945729978\n",
      "  (0, 4367)\t0.09170756872463914\n",
      "  (0, 428)\t0.08667917992712185\n",
      "  (0, 4080)\t0.06969946394633159\n",
      "  (0, 3237)\t0.06667790837944568\n",
      "  (0, 4352)\t0.04534373288791082\n",
      "  (0, 185)\t0.08981081019560776\n",
      "  (0, 1951)\t0.10284607944589544\n",
      "  (0, 4593)\t0.03740601351950423\n",
      "  (0, 4191)\t0.051340736589156465\n",
      "  (0, 473)\t0.0579746097114602\n",
      "  (0, 4780)\t0.11795505561919196\n",
      "  (0, 629)\t0.024148643168897643\n",
      "  (0, 4360)\t0.05095658664716226\n",
      "  (0, 3882)\t0.05127314598344674\n",
      "  (0, 414)\t0.028512004529717583\n",
      "  (0, 1303)\t0.058888382653347796\n",
      "  (0, 1301)\t0.04273938889058725\n",
      "  (0, 2622)\t0.0940048296445188\n",
      "  (0, 4508)\t0.0643172197958261\n",
      "  (0, 3992)\t0.0641154224442384\n",
      "  (0, 2542)\t0.048993637665276535\n",
      "  (0, 142)\t0.044753138386484564\n",
      "  (0, 4600)\t0.058183967358789175\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 42 stored elements and shape (1, 4898)>\n",
      "  Coords\tValues\n",
      "  (0, 185)\t0.059303898024568674\n",
      "  (0, 188)\t0.1938056972145902\n",
      "  (0, 191)\t0.11660141498833995\n",
      "  (0, 223)\t0.07440195319703746\n",
      "  (0, 554)\t0.22065865816828922\n",
      "  (0, 702)\t0.10965840047394565\n",
      "  (0, 999)\t0.08752714812548403\n",
      "  (0, 1301)\t0.08466505384639297\n",
      "  (0, 1626)\t0.14573936011659516\n",
      "  (0, 1720)\t0.5226651959115426\n",
      "  (0, 1880)\t0.22558376677342606\n",
      "  (0, 1898)\t0.10795888094817585\n",
      "  (0, 2019)\t0.05636431221925884\n",
      "  (0, 2023)\t0.06768253819915672\n",
      "  (0, 2081)\t0.13316206873388065\n",
      "  (0, 2142)\t0.08555022687068456\n",
      "  (0, 2317)\t0.03988471290955115\n",
      "  (0, 2444)\t0.19106230438810856\n",
      "  (0, 2777)\t0.15474625193493063\n",
      "  (0, 2814)\t0.3227521689803976\n",
      "  (0, 3027)\t0.03785906255864161\n",
      "  (0, 3048)\t0.052690823623864874\n",
      "  (0, 3050)\t0.1128606235666483\n",
      "  (0, 3084)\t0.06680407718778991\n",
      "  (0, 3170)\t0.12219699642295298\n",
      "  (0, 3171)\t0.1342076629060087\n",
      "  (0, 3245)\t0.15593766436746745\n",
      "  (0, 3435)\t0.1118667172600954\n",
      "  (0, 3596)\t0.13379072015752708\n",
      "  (0, 3796)\t0.14626011441930767\n",
      "  (0, 3884)\t0.10827953697152432\n",
      "  (0, 3974)\t0.06324985838735492\n",
      "  (0, 4081)\t0.03597736766036832\n",
      "  (0, 4327)\t0.141444437507149\n",
      "  (0, 4346)\t0.1088294651265243\n",
      "  (0, 4372)\t0.15819765394693208\n",
      "  (0, 4422)\t0.07648336498729644\n",
      "  (0, 4578)\t0.25347875173683904\n",
      "  (0, 4714)\t0.05166592529864719\n",
      "  (0, 4716)\t0.1281669484185261\n",
      "  (0, 4843)\t0.10652981951688663\n",
      "  (0, 4888)\t0.057845028701745876\n",
      "start 0.018161574791954474\n",
      "this 0.05989429002682193\n",
      "film 0.17260869183197405\n",
      "was 0.2868937029805583\n",
      "just 0.13532479702216973\n",
      "brilliant 0.2283618610749408\n",
      "casting 0.08695901899640085\n",
      "location 0.09754452672825938\n",
      "scenery 0.09391048358927476\n",
      "story 0.07964733966197848\n",
      "direction 0.0721956312972466\n",
      "everyone 0.12936111313919155\n",
      "really 0.07943724066926205\n",
      "suited 0.11654538443742532\n",
      "the 0.2746885888265036\n",
      "part 0.05562279686636129\n",
      "they 0.13333473515845054\n",
      "played 0.12596764917385855\n",
      "and 0.16901333435097304\n",
      "you 0.11680196561648677\n",
      "could 0.04418418996113909\n",
      "imagine 0.08328246387682232\n",
      "being 0.0945671688358273\n",
      "there 0.06679152646353043\n",
      "robert 0.08132183922770295\n",
      "unk 0.2010760402177639\n",
      "is 0.020134024351103112\n",
      "an 0.031178746211349546\n",
      "amazing 0.2242295466295999\n",
      "actor 0.06475599416421257\n",
      "now 0.052767747422336454\n",
      "same 0.10960314786640192\n",
      "director 0.053643113210144665\n",
      "father 0.0700881628135954\n",
      "came 0.06882754092137516\n",
      "from 0.03192732919695216\n",
      "scottish 0.12415636939126398\n",
      "island 0.09553357104292177\n",
      "as 0.0783990028819672\n",
      "myself 0.07546138863407935\n",
      "so 0.12771551765838282\n",
      "loved 0.07261315715191546\n",
      "fact 0.056846041978940935\n",
      "real 0.05284114739342268\n",
      "connection 0.10276928620339686\n",
      "with 0.049348719566806004\n",
      "witty 0.1011571209749401\n",
      "remarks 0.12035423954626456\n",
      "throughout 0.07229553805098785\n",
      "were 0.08376551846426686\n",
      "great 0.04317758694794135\n",
      "it 0.12155006431166045\n",
      "much 0.041062157372553594\n",
      "that 0.08772091993650913\n",
      "bought 0.0923341412742853\n",
      "soon 0.07535945249299239\n",
      "released 0.07855585891172027\n",
      "for 0.07274066397688321\n",
      "would 0.03855900159282529\n",
      "recommend 0.06813161000140915\n",
      "to 0.05791384044561235\n",
      "watch 0.04577386531238084\n",
      "fly 0.10585208229945424\n",
      "cried 0.11704299295636558\n",
      "at 0.060239691093171135\n",
      "end 0.049149018983103844\n",
      "sad 0.0786774770545289\n",
      "know 0.04822465561995873\n",
      "what 0.06986964051381622\n",
      "say 0.049971616839670475\n",
      "if 0.033593476675479544\n",
      "cry 0.09589733434373411\n",
      "must 0.058249889662717214\n",
      "have 0.05690603501794178\n",
      "been 0.042202008600288665\n",
      "good 0.03551284292555395\n",
      "definitely 0.07034342456717599\n",
      "also 0.04273938889058725\n",
      "two 0.04753078340753402\n",
      "little 0.047994161303423745\n",
      "of 0.038222929631923704\n",
      "norman 0.12342493195990666\n",
      "paul 0.08489849977616434\n",
      "children 0.1498153807053939\n",
      "are 0.08646480817321811\n",
      "often 0.07064148064677406\n",
      "left 0.06558737690672517\n",
      "out 0.03372306878332425\n",
      "list 0.08853344945729978\n",
      "think 0.09170756872463914\n",
      "because 0.08667917992712185\n",
      "stars 0.06969946394633159\n",
      "play 0.06667790837944568\n",
      "them 0.04534373288791082\n",
      "all 0.08981081019560776\n",
      "grown 0.10284607944589544\n",
      "up 0.03740601351950423\n",
      "such 0.051340736589156465\n",
      "big 0.0579746097114602\n",
      "whole 0.11795505561919196\n",
      "but 0.024148643168897643\n",
      "these 0.05095658664716226\n",
      "should 0.05127314598344674\n",
      "be 0.028512004529717583\n",
      "done 0.058888382653347796\n",
      "don 0.04273938889058725\n",
      "lovely 0.0940048296445188\n",
      "true 0.0643172197958261\n",
      "someone 0.0641154224442384\n",
      "life 0.048993637665276535\n",
      "after 0.044753138386484564\n",
      "us 0.058183967358789175\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nlog_reg = LogisticRegression(max_iter=200)\\nlog_reg = LogisticRegression()\\nlog_reg.fit(X_train_vec, y_train)\\ny_pred_log_reg = log_reg.predict(X_test_vec)\\nprint(classification_report(y_test, y_pred_log_reg, target_names=[\"Negative\", \"Positive\"]))\\n\\n\\n\\nnaive_bayes = MultinomialNB()\\nnaive_bayes.fit(X_train_vec, y_train)\\ny_pred_nb = naive_bayes.predict(X_test_vec)\\nprint(classification_report(y_test, y_pred_nb, target_names=[\"Negative\", \"Positive\"]))\\n\\n\\n\\nsvm = LinearSVC()\\nsvm.fit(X_train_vec, y_train)\\ny_pred_svm = svm.predict(X_test_vec)\\nprint(classification_report(y_test, y_pred_svm, target_names=[\"Negative\", \"Positive\"]))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "max_features = 5000\n",
    "maxlen = 500\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Decode function to convert sequences back to text\n",
    "word_index = imdb.get_word_index()\n",
    "index_word = {v + 3: k for k, v in word_index.items()}\n",
    "index_word[0] = '<PAD>'\n",
    "index_word[1] = '<START>'\n",
    "index_word[2] = '<UNK>'\n",
    "\n",
    "\n",
    "X_train_text = [' '.join(index_word.get(i, '?') for i in seq) for seq in X_train]\n",
    "X_test_text = [' '.join(index_word.get(i, '?') for i in seq) for seq in X_test]\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "print(X_train_vec[0])\n",
    "print(X_test_vec[0])\n",
    "\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred_log_reg, target_names=[\"Negative\", \"Positive\"]))\n",
    "\n",
    "\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_vec, y_train)\n",
    "y_pred_nb = naive_bayes.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred_nb, target_names=[\"Negative\", \"Positive\"]))\n",
    "\n",
    "\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_vec, y_train)\n",
    "y_pred_svm = svm.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred_svm, target_names=[\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOi161KmoA9b"
   },
   "source": [
    "# **CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-glkGJHtAPwK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parameters from your paper\n",
    "vocab_size = 5000\n",
    "embedding_dim = 32\n",
    "max_sequence_length = 500\n",
    "kernel_sizes = [3, 5, 7, 9]  # Multiple branches\n",
    "num_filters = 128\n",
    "pool_size = 2\n",
    "lstm_units = 128\n",
    "dropout_rate = 0.5\n",
    "\n",
    "class MultiBranchCNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiBranchCNN_LSTM, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Create branches for each kernel size\n",
    "        self.branches = nn.ModuleList()\n",
    "        for k in kernel_sizes:\n",
    "            branch = nn.ModuleDict({\n",
    "                'conv': nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k, padding='same'),\n",
    "                'pool': nn.MaxPool1d(kernel_size=pool_size),\n",
    "                'dropout': nn.Dropout(dropout_rate),\n",
    "                'batch_norm': nn.BatchNorm1d(num_filters),\n",
    "                'lstm': nn.LSTM(input_size=num_filters, hidden_size=lstm_units, batch_first=True)\n",
    "            })\n",
    "            self.branches.append(branch)\n",
    "\n",
    "        # Final dense layer\n",
    "        self.fc = nn.Linear(lstm_units * len(kernel_sizes), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x = self.embedding(x)                   # [batch_size, seq_len, embedding_dim]\n",
    "        x = x.permute(0, 2, 1)                 # [batch_size, embedding_dim, seq_len] for Conv1d\n",
    "\n",
    "        branch_outputs = []\n",
    "        for branch in self.branches:\n",
    "            out = F.relu(branch['conv'](x))      # Conv + ReLU\n",
    "            out = branch['pool'](out)            # MaxPooling\n",
    "            out = branch['dropout'](out)         # Dropout\n",
    "            out = branch['batch_norm'](out)      # BatchNorm\n",
    "\n",
    "            out = out.permute(0, 2, 1)           # [batch_size, seq_len//pool, num_filters] for LSTM\n",
    "            out, (h_n, c_n) = branch['lstm'](out)\n",
    "            out = out[:, -1, :]                  # Take last timestep\n",
    "            branch_outputs.append(out)\n",
    "\n",
    "        # Concatenate all branch outputs\n",
    "        out = torch.cat(branch_outputs, dim=1)   # [batch_size, lstm_units * num_branches]\n",
    "        out = torch.sigmoid(self.fc(out))        # Final output\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "model = MultiBranchCNN_LSTM()\n",
    "print(model)\n",
    "\n",
    "# Example input\n",
    "batch_size = 32\n",
    "dummy_input = torch.randint(0, vocab_size, (batch_size, max_sequence_length))\n",
    "output = model(dummy_input)\n",
    "print(output.shape)  # Should be [32, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgGPTWpioFnS"
   },
   "source": [
    "# **Additional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KslEbQ-y6ZKA",
    "outputId": "280db82a-0d89-4c06-e505-1967da7adb22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(len(X_train))\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "first_doc = X_train_vec[0]\n",
    "for idx in first_doc.nonzero()[1]:\n",
    "    print(feature_names[idx], first_doc[0, idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mulztC9zcbJ5",
    "outputId": "afb5ea12-af0d-43c3-db24-2af355f6d352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> please give this one a miss br br <UNK> <UNK> and the rest of the cast <UNK> terrible performances the show is flat flat flat br br i don't know how michael <UNK> could have allowed this one on his <UNK> he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you <UNK> fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "# Rebuild index-word dictionary if you haven't already\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "index_word = {v + 3: k for k, v in word_index.items()}\n",
    "index_word[0] = \"<PAD>\"\n",
    "index_word[1] = \"<START>\"\n",
    "index_word[2] = \"<UNK>\"\n",
    "index_word[3] = \"<UNUSED>\"\n",
    "\n",
    "# Example: decode the first X_test review\n",
    "decoded_review = ' '.join([index_word.get(i, '?') for i in X_test[0]])\n",
    "print(decoded_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5pFs2IwsdEYj",
    "outputId": "92f12d92-b06d-4a83-d04c-bb58526d7c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:\n",
      " <START> please give this one a miss br br <UNK> <UNK> and the rest of the cast <UNK> terrible performances the show is flat flat flat br br i don't know how michael <UNK> could have allowed this one on his <UNK> he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you <UNK> fans give this a miss\n",
      "\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# Decode integer-encoded review\n",
    "decoded_review = ' '.join([index_word.get(i, '?') for i in X_test[0]])\n",
    "print(\"Review:\\n\", decoded_review)\n",
    "\n",
    "# Convert to TF-IDF vector\n",
    "review_tfidf = vectorizer.transform([decoded_review])\n",
    "\n",
    "# Predict sentiment with a model (e.g., logistic regression)\n",
    "log_reg_pred = log_reg.predict(review_tfidf)[0]\n",
    "naive_bayes_pred = naive_bayes.predict(review_tfidf)[0]\n",
    "svm_pred = svm.predict(review_tfidf)[0]\n",
    "\n",
    "log_reg_label = \"Positive\" if log_reg_pred == 1 else \"Negative\"\n",
    "naive_bayes_label = \"Positive\" if naive_bayes_pred == 1 else \"Negative\"\n",
    "svm_label = \"Positive\" if svm_pred == 1 else \"Negative\"\n",
    "\n",
    "print(\"\\nPredicted Sentiment:\", log_reg_label)\n",
    "print(\"\\nPredicted Sentiment:\", naive_bayes_label)\n",
    "print(\"\\nPredicted Sentiment:\", svm_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHi6lFsje4IT",
    "outputId": "6c86a2be-526b-4561-e539-1320ddea5912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top contributing words to sentiment:\n",
      "miss (weight: 1.055)\n",
      "performances (weight: 1.771)\n",
      "and (weight: 2.873)\n",
      "his (weight: 1.400)\n",
      "know (weight: 0.891)\n",
      "performance (weight: 1.351)\n",
      "fans (weight: 0.983)\n",
      "you (weight: 2.463)\n",
      "quite (weight: 1.267)\n",
      "the (weight: 0.793)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the coefficients from the trained logistic regression model\n",
    "coefs = log_reg.coef_[0]  # 1D array of weights\n",
    "\n",
    "# Get indices of non-zero TF-IDF terms for this review\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_vector = review_tfidf.toarray()[0]\n",
    "\n",
    "# Sort features by their contribution to sentiment\n",
    "top_indices = np.argsort(tfidf_vector * coefs)[::-1][:10]\n",
    "\n",
    "print(\"\\nTop contributing words to sentiment:\")\n",
    "for idx in top_indices:\n",
    "    if tfidf_vector[idx] > 0:\n",
    "        print(f\"{feature_names[idx]} (weight: {coefs[idx]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k32Z1dZhgbnI"
   },
   "source": [
    "# **Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JSz9G8Ue-Vm",
    "outputId": "210e8403-bc15-46b7-c881-94b10ad8fa63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top contributing words to sentiment (Naive Bayes): \n",
      "\n",
      "miss (log-odds: 0.452)\n",
      "performances (log-odds: 0.750)\n",
      "performance (log-odds: 0.639)\n",
      "michael (log-odds: 0.347)\n",
      "his (log-odds: 0.313)\n",
      "fans (log-odds: 0.258)\n",
      "quite (log-odds: 0.286)\n",
      "cast (log-odds: 0.239)\n",
      "show (log-odds: 0.209)\n",
      "unk (log-odds: 0.080)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the log odds (positive - negative)\n",
    "coefs = naive_bayes.feature_log_prob_[1] - naive_bayes.feature_log_prob_[0]\n",
    "\n",
    "# Convert review to TF-IDF vector\n",
    "review_tfidf = vectorizer.transform([decoded_review])\n",
    "tfidf_vector = review_tfidf.toarray()[0]\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Compute weighted contributions\n",
    "contributions = tfidf_vector * coefs\n",
    "top_indices = np.argsort(contributions)[::-1][:10]\n",
    "\n",
    "print(\"Top contributing words to sentiment (Naive Bayes): \\n\")\n",
    "for idx in top_indices:\n",
    "    if tfidf_vector[idx] > 0:\n",
    "        print(f\"{feature_names[idx]} (log-odds: {coefs[idx]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o_1GL-NgV9q"
   },
   "source": [
    "# **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_WLCgoxfUBh",
    "outputId": "e8ee9879-f8b1-47f8-dd7d-bc828fbc1834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top contributing words to sentiment:\n",
      "\n",
      "miss (weight: 0.668)\n",
      "performances (weight: 1.015)\n",
      "fans (weight: 0.665)\n",
      "know (weight: 0.392)\n",
      "and (weight: 0.985)\n",
      "his (weight: 0.548)\n",
      "quite (weight: 0.633)\n",
      "you (weight: 1.098)\n",
      "performance (weight: 0.468)\n",
      "the (weight: 0.502)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the coefficients from the trained logistic regression model\n",
    "coefs = svm.coef_[0]  # 1D array of weights\n",
    "\n",
    "# Get indices of non-zero TF-IDF terms for this review\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_vector = review_tfidf.toarray()[0]\n",
    "\n",
    "# Sort features by their contribution to sentiment\n",
    "top_indices = np.argsort(tfidf_vector * coefs)[::-1][:10]\n",
    "\n",
    "print(\"Top contributing words to sentiment:\\n\")\n",
    "for idx in top_indices:\n",
    "    if tfidf_vector[idx] > 0:\n",
    "        print(f\"{feature_names[idx]} (weight: {coefs[idx]:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
