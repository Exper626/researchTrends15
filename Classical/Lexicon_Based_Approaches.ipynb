{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk gensim vaderSentiment numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NvYQofwo8sO",
        "outputId": "8733f8b9-93e6-46f8-a51d-ffc87ecc58bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment, gensim\n",
            "Successfully installed gensim-4.4.0 vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install nltk gensim vaderSentiment numpy\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Changed to 'averaged_perceptron_tagger_eng' as suggested by error\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# ------------------------\n",
        "# Lexicon-based Functions\n",
        "# ------------------------\n",
        "\n",
        "# VADER analyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(text):\n",
        "    return vader_analyzer.polarity_scores(text)\n",
        "\n",
        "# SentiWordNet analyzer\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def sentiwordnet_sentiment(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    sentiment_score = 0\n",
        "    count = 0\n",
        "    for word, tag in tagged:\n",
        "        wn_tag = get_wordnet_pos(tag)\n",
        "        if wn_tag:\n",
        "            synsets = list(wordnet.synsets(word, pos=wn_tag))\n",
        "            if synsets:\n",
        "                synset = synsets[0]\n",
        "                swn_synset = swn.senti_synset(synset.name())\n",
        "                sentiment_score += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "                count += 1\n",
        "    return sentiment_score / count if count != 0 else 0\n",
        "\n",
        "# ------------------------\n",
        "# Traditional Embedding Models\n",
        "# ------------------------\n",
        "\n",
        "# Sample sentences for training Word2Vec & FastText\n",
        "sample_texts = [\n",
        "    \"I love machine learning\",\n",
        "    \"This product is terrible\",\n",
        "    \"The movie was fantastic\",\n",
        "    \"I'm not happy with this service\",\n",
        "    \"Absolutely amazing experience\"\n",
        "]\n",
        "\n",
        "tokenized_texts = [word_tokenize(sent.lower()) for sent in sample_texts]\n",
        "\n",
        "# Train Word2Vec\n",
        "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Train FastText\n",
        "fasttext_model = FastText(sentences=tokenized_texts, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "# Download glove.6B.50d.txt from https://nlp.stanford.edu/projects/glove/\n",
        "glove_file = 'glove.6B.50d.txt'\n",
        "\n",
        "# Download GloVe file if not present\n",
        "import os\n",
        "if not os.path.exists(glove_file):\n",
        "    print(f\"Downloading {glove_file}...\")\n",
        "    !wget -P . https://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip -o glove.6B.zip\n",
        "\n",
        "glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
        "\n",
        "# Function to compute sentence embedding\n",
        "def sentence_embedding(tokens, model):\n",
        "    vecs = [model[word] for word in tokens if word in model]\n",
        "    if vecs:\n",
        "        return np.mean(vecs, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# ------------------------\n",
        "# Full Pipeline Function\n",
        "# ------------------------\n",
        "def analyze_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    result = {\n",
        "        'text': text,\n",
        "        'vader': vader_sentiment(text),\n",
        "        'sentiwordnet': sentiwordnet_sentiment(text),\n",
        "        'word2vec_embedding': sentence_embedding(tokens, word2vec_model.wv),\n",
        "        'fasttext_embedding': sentence_embedding(tokens, fasttext_model.wv),\n",
        "        'glove_embedding': sentence_embedding(tokens, glove_model)\n",
        "    }\n",
        "    return result\n",
        "\n",
        "# ------------------------\n",
        "# Example Usage\n",
        "# ------------------------\n",
        "texts = [\n",
        "    \"I love this product! It's amazing üòä\",\n",
        "    \"The service was terrible and I hate it\",\n",
        "    \"I'm not sure how I feel about this\"\n",
        "]\n",
        "\n",
        "for t in texts:\n",
        "    output = analyze_text(t)\n",
        "    print(f\"Text: {output['text']}\")\n",
        "    print(f\"VADER: {output['vader']}\")\n",
        "    print(f\"SentiWordNet: {output['sentiwordnet']}\")\n",
        "    print(f\"Word2Vec Embedding Shape: {output['word2vec_embedding'].shape}\")\n",
        "    print(f\"FastText Embedding Shape: {output['fasttext_embedding'].shape}\")\n",
        "    print(f\"GloVe Embedding Shape: {output['glove_embedding'].shape}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiZUrfMpo7O7",
        "outputId": "75820f10-6389-4d52-a943-febf03332ccf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.12/dist-packages (3.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love this product! It's amazing üòä\n",
            "VADER: {'neg': 0.0, 'neu': 0.329, 'pos': 0.671, 'compound': 0.9359}\n",
            "SentiWordNet: 0.25\n",
            "Word2Vec Embedding Shape: (50,)\n",
            "FastText Embedding Shape: (50,)\n",
            "GloVe Embedding Shape: (50,)\n",
            "\n",
            "Text: The service was terrible and I hate it\n",
            "VADER: {'neg': 0.531, 'neu': 0.469, 'pos': 0.0, 'compound': -0.7783}\n",
            "SentiWordNet: -0.3125\n",
            "Word2Vec Embedding Shape: (50,)\n",
            "FastText Embedding Shape: (50,)\n",
            "GloVe Embedding Shape: (50,)\n",
            "\n",
            "Text: I'm not sure how I feel about this\n",
            "VADER: {'neg': 0.219, 'neu': 0.781, 'pos': 0.0, 'compound': -0.2411}\n",
            "SentiWordNet: -0.3333333333333333\n",
            "Word2Vec Embedding Shape: (50,)\n",
            "FastText Embedding Shape: (50,)\n",
            "GloVe Embedding Shape: (50,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xg5GZWrPo8vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cEQ7XT1FtpUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset from T4SA\n",
        "!wget --user=t4sa --password=U4Cm_dUa http://www.t4sa.it/dataset/t4sa_text_sentiment.tsv\n",
        "!wget --user=t4sa --password=U4Cm_dUa http://www.t4sa.it/dataset/raw_tweets_text.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "sent_df = pd.read_csv('t4sa_text_sentiment.tsv', sep='\\t')\n",
        "sent_df['TWID'] = sent_df['TWID'].astype(str)\n",
        "\n",
        "text_df = pd.read_csv('raw_tweets_text.csv')\n",
        "text_df['id'] = text_df['id'].astype(str)\n",
        "\n",
        "# Merge on IDs\n",
        "merged_df = pd.merge(sent_df, text_df, left_on='TWID', right_on='id', how='inner')\n",
        "merged_df = merged_df.drop(columns=['id'])\n",
        "\n",
        "# Assign numeric label: NEG=0, NEU=1, POS=2\n",
        "merged_df['label'] = merged_df[['NEG', 'NEU', 'POS']].idxmax(axis=1).map({\n",
        "    'NEG': 0,\n",
        "    'NEU': 1,\n",
        "    'POS': 2\n",
        "})\n",
        "\n",
        "# Rename columns\n",
        "merged_df = merged_df.rename(columns={'TWID': 'twitter_id', 'text': 'content'})\n",
        "\n",
        "# Final dataset\n",
        "final_df = merged_df[['twitter_id', 'label', 'content', 'NEG', 'NEU', 'POS']]\n",
        "\n",
        "# Preview\n",
        "print(\"Dataset size:\", len(final_df))\n",
        "print(\"\\nFirst 20 rows (twitter_id, label, content):\")\n",
        "print(final_df[['twitter_id', 'label', 'content']].head(20))\n",
        "\n",
        "print(\"\\nSentiment label distribution:\")\n",
        "print(final_df['label'].value_counts().sort_index())\n",
        "\n",
        "# Save merged dataset\n",
        "final_df.to_csv('t4sa_merged_text_sentiment.csv', index=False)\n",
        "print(\"\\nMerged dataset saved as 't4sa_merged_text_sentiment.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmVd6MCMo1D5",
        "outputId": "ec5a2564-36e3-4cff-b430-b4ae8afa810e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-14 07:15:12--  http://www.t4sa.it/dataset/t4sa_text_sentiment.tsv\n",
            "Resolving www.t4sa.it (www.t4sa.it)... 146.48.85.151\n",
            "Connecting to www.t4sa.it (www.t4sa.it)|146.48.85.151|:80... connected.\n",
            "HTTP request sent, awaiting response... 401 Authorization Required\n",
            "Authentication selected: Basic realm=\"T4SA Dataset\"\n",
            "Reusing existing connection to www.t4sa.it:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77271921 (74M) [text/tab-separated-values]\n",
            "Saving to: ‚Äòt4sa_text_sentiment.tsv‚Äô\n",
            "\n",
            "t4sa_text_sentiment 100%[===================>]  73.69M  17.9MB/s    in 7.4s    \n",
            "\n",
            "2025-12-14 07:15:20 (9.95 MB/s) - ‚Äòt4sa_text_sentiment.tsv‚Äô saved [77271921/77271921]\n",
            "\n",
            "--2025-12-14 07:15:20--  http://www.t4sa.it/dataset/raw_tweets_text.csv\n",
            "Resolving www.t4sa.it (www.t4sa.it)... 146.48.85.151\n",
            "Connecting to www.t4sa.it (www.t4sa.it)|146.48.85.151|:80... connected.\n",
            "HTTP request sent, awaiting response... 401 Authorization Required\n",
            "Authentication selected: Basic realm=\"T4SA Dataset\"\n",
            "Reusing existing connection to www.t4sa.it:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 433484362 (413M) [text/csv]\n",
            "Saving to: ‚Äòraw_tweets_text.csv‚Äô\n",
            "\n",
            "raw_tweets_text.csv 100%[===================>] 413.40M  18.1MB/s    in 27s     \n",
            "\n",
            "2025-12-14 07:15:47 (15.6 MB/s) - ‚Äòraw_tweets_text.csv‚Äô saved [433484362/433484362]\n",
            "\n",
            "Dataset size: 1179957\n",
            "\n",
            "First 20 rows (twitter_id, label, content):\n",
            "            twitter_id  label  \\\n",
            "0   768096868504969216      1   \n",
            "1   768097237620490241      1   \n",
            "2   768097619281227776      2   \n",
            "3   768097619285536768      1   \n",
            "4   768097627686604801      2   \n",
            "5   768097627686727680      1   \n",
            "6   768097627695042560      2   \n",
            "7   768097631864102912      1   \n",
            "8   768097631872618497      1   \n",
            "9   768097636075368448      1   \n",
            "10  768097636087980032      2   \n",
            "11  768097640269643776      2   \n",
            "12  768097640278089729      2   \n",
            "13  768097644451278852      1   \n",
            "14  768097644463788032      1   \n",
            "15  768097644480573443      1   \n",
            "16  768097648675028992      2   \n",
            "17  768097652860792833      1   \n",
            "18  768097661237026816      2   \n",
            "19  768097665418747908      2   \n",
            "\n",
            "                                              content  \n",
            "0   #Incredible #India #Atulya #Bharat - Land of S...  \n",
            "1   RT @AlwaysTrustKay: Are you near a Western uni...  \n",
            "2   RT @KendallHuntRPD: The #firstdayofschool for ...  \n",
            "3   RT @abbiesf_: Kate wrights figure is all I wan...  \n",
            "4   Josh Jenkins is looking forward to TAB Breeder...  \n",
            "5   Robert Pattinson Gets Ready to Hop on a Plane ...  \n",
            "6   RT @PEPalerts: This September, @YESmag is taki...  \n",
            "7   RT @2pmthailfans: [Pic] Nichkhun from krjeong8...  \n",
            "8   RT @cigdemk14: This kid is 11 and in my organi...  \n",
            "9   RT @Letestpic: Chicken Wings, Chicken Tenders ...  \n",
            "10  RT @SH4WNSMILE: -Who is excited for illuminate...  \n",
            "11  RT @BantySrkian: #SRK and kajol in the making ...  \n",
            "12  RT @MianUsmanJaved: Congratulations Pakistan o...  \n",
            "13  RT @OnTopApparel: Just dropped some new heat t...  \n",
            "14  RT @GregAbbott_TX: Women owned business in Tex...  \n",
            "15  RT @OnTopApparel: Just dropped some new heat t...  \n",
            "16  Local Directory Thank You - Influence Engine M...  \n",
            "17  RT @androidcentral: This is our first official...  \n",
            "18  RT @david_gaibis: Newly painted walls, thanks ...  \n",
            "19  RT @CedricFeschotte: Excited to announce: as o...  \n",
            "\n",
            "Sentiment label distribution:\n",
            "label\n",
            "0    179050\n",
            "1    629566\n",
            "2    371341\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Merged dataset saved as 't4sa_merged_text_sentiment.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install nltk gensim vaderSentiment numpy pandas tabulate\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import os\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# ------------------------\n",
        "# Lexicon-based Functions\n",
        "# ------------------------\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(text):\n",
        "    scores = vader_analyzer.polarity_scores(text)\n",
        "    # Convert compound score to discrete label: 0=NEG, 1=NEU, 2=POS\n",
        "    compound = scores['compound']\n",
        "    if compound > 0.05:\n",
        "        label = 2\n",
        "    elif compound < -0.05:\n",
        "        label = 0\n",
        "    else:\n",
        "        label = 1\n",
        "    return label\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return None\n",
        "\n",
        "def sentiwordnet_sentiment(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    sentiment_score = 0\n",
        "    count = 0\n",
        "    for word, tag in tagged:\n",
        "        wn_tag = get_wordnet_pos(tag)\n",
        "        if wn_tag:\n",
        "            synsets = list(wordnet.synsets(word, pos=wn_tag))\n",
        "            if synsets:\n",
        "                swn_synset = swn.senti_synset(synsets[0].name())\n",
        "                sentiment_score += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "                count += 1\n",
        "    avg_score = sentiment_score / count if count != 0 else 0\n",
        "    # Convert to discrete label\n",
        "    if avg_score > 0.05:\n",
        "        return 2\n",
        "    elif avg_score < -0.05:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# ------------------------\n",
        "# Traditional Embedding Models\n",
        "# ------------------------\n",
        "# Load merged dataset\n",
        "df = pd.read_csv('t4sa_merged_text_sentiment.csv')\n",
        "sample_texts = df['content'].tolist()\n",
        "tokenized_texts = [word_tokenize(sent.lower()) for sent in sample_texts]\n",
        "\n",
        "# Word2Vec & FastText\n",
        "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=50, window=5, min_count=1, workers=4)\n",
        "fasttext_model = FastText(sentences=tokenized_texts, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# GloVe embeddings\n",
        "glove_file = 'glove.6B.50d.txt'\n",
        "if not os.path.exists(glove_file):\n",
        "    print(f\"Downloading {glove_file}...\")\n",
        "    !wget -P . https://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip -o glove.6B.zip\n",
        "\n",
        "glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
        "\n",
        "def sentence_embedding(tokens, model):\n",
        "    vecs = [model[word] for word in tokens if word in model]\n",
        "    if vecs:\n",
        "        return np.mean(vecs, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "def embedding_sentiment(tokens, model):\n",
        "    vec = sentence_embedding(tokens, model)\n",
        "    # For evaluation, use simple heuristic: sum(vector)>0 ‚Üí POS, <0 ‚Üí NEG, else NEU\n",
        "    s = vec.sum()\n",
        "    if s > 0.05:\n",
        "        return 2\n",
        "    elif s < -0.05:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# ------------------------\n",
        "# Evaluation Table\n",
        "# ------------------------\n",
        "results = []\n",
        "for text, true_label in zip(df['content'][:50], df['label'][:50]):  # Limit to first 50 for speed\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    vader_label = vader_sentiment(text)\n",
        "    swn_label = sentiwordnet_sentiment(text)\n",
        "    w2v_label = embedding_sentiment(tokens, word2vec_model.wv)\n",
        "    ft_label = embedding_sentiment(tokens, fasttext_model.wv)\n",
        "    glove_label = embedding_sentiment(tokens, glove_model)\n",
        "\n",
        "    results.append([text, true_label, vader_label, swn_label, w2v_label, ft_label, glove_label])\n",
        "\n",
        "# Display as table\n",
        "headers = ['Text', 'True', 'VADER', 'SentiWordNet', 'Word2Vec', 'FastText', 'GloVe']\n",
        "print(tabulate(results, headers=headers, tablefmt='grid'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irM0eFBMtqNb",
        "outputId": "ca3c8132-ccba-47c8-d771-9b5de2532286"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.12/dist-packages (3.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Text                                                                                                                                                      |   True |   VADER |   SentiWordNet |   Word2Vec |   FastText |   GloVe |\n",
            "+===========================================================================================================================================================+========+=========+================+============+============+=========+\n",
            "| #Incredible #India #Atulya #Bharat - Land of Seekers #BeProud üôè üáÆüá≥  :|: Plz RT https://t.co/vpghReZWsa                                                   |      1 |       2 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @AlwaysTrustKay: Are you near a Western union &amp; want to make up to $3000+ today? Hit me up let me walk you through the process ???????????? https‚Ä¶ |      1 |       2 |              1 |          0 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @KendallHuntRPD: The #firstdayofschool for students &amp; teachers. Good luck and have a Successful 2016-17 School Year #EducationMatters ht‚Ä¶          |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @abbiesf_: Kate wrights figure is all I want in life üî• https://t.co/0AtCwSKo2w                                                                        |      1 |       0 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Josh Jenkins is looking forward to TAB Breeders Crown Super Sunday https://t.co/antImqAo4Y https://t.co/ejnA78Sks0                                        |      2 |       2 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Robert Pattinson Gets Ready to Hop on a Plane Out of LAX Airport https://t.co/dV7ABLcVj9 https://t.co/yVf8dzvqoe                                          |      1 |       2 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @PEPalerts: This September, @YESmag is taking you to Maine Mendoza‚Äôs surprise thanksgiving party she threw for her fans! https://t.co/oX‚Ä¶              |      2 |       2 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @2pmthailfans: [Pic] Nichkhun from krjeong86's IG https://t.co/5gcAcu9by7                                                                              |      1 |       1 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @cigdemk14: This kid is 11 and in my organic chemistry class üò≠ he said if we have questions to just email him https://t.co/oz46on8ywP                 |      1 |       0 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @Letestpic: Chicken Wings, Chicken Tenders &amp;amp; Mozzarella Sticks https://t.co/pOeT6SuC6k                                                         |      1 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @SH4WNSMILE: -Who is excited for illuminate !? -.... https://t.co/AKnIVEGUOe                                                                           |      2 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @BantySrkian: #SRK and kajol in the making of DDLJ song .. Both are greatest ever üôèüôèüôè https://t.co/Aa7fAvvtNE                                       |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @MianUsmanJaved: Congratulations Pakistan on becoming #No1TestTeam in the world against all odds! #JI_PakZindabadRallies https://t.co/1o‚Ä¶              |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @OnTopApparel: Just dropped some new heat to the website ????6!X Dad Hat ???? https://t.co/1qSIFSCqvQ                                                  |      1 |       1 |              1 |          0 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @GregAbbott_TX: Women owned business in Texas are No. 1 in the nation in economic clout. https://t.co/vk3srGEAwB                                       |      1 |       1 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @OnTopApparel: Just dropped some new heat to the website ????6!X Dad Hat ???? https://t.co/1qSIFSCqvQ                                                  |      1 |       1 |              1 |          0 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Local Directory Thank You - Influence Engine Marketer #Gaming https://t.co/ALhY5c9E2k                                                                     |      2 |       2 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @androidcentral: This is our first official look at the LG V20, courtesy of Googlehttps://t.co/3wKngbuBEE https://t.co/6GfGmpj6Gg                      |      1 |       2 |              2 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @david_gaibis: Newly painted walls, thanks a million to our custodial painters this summer.  Great job ladies!!!#EC_proud https://t.co/‚Ä¶               |      2 |       2 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @CedricFeschotte: Excited to announce: as of July 2017 Feschotte lab will be relocating to @Cornell MBG https://t.co/dd0FG7BRx3                        |      2 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Perennials across Europe: Czech, Slovak, German https://t.co/iUNMPYNxgH https://t.co/O5Owl5GjBo                                                           |      1 |       1 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @Splatland: \"Spellbindingly dark and suspenseful\" @ChantiReviewsGHOSTS OF MATEGUAS#ASMSG #RRBChttps://t.co/FKnNZ4Qp8I https://t.co/we‚Ä¶                 |      1 |       1 |              0 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @SCastelloXXX: bestfriend: BIIIITTTTCCCHHHHH GUESS WHO JUST TRIED TO MESSAGE ME!!!! me: BITCHHHHHH WHOOOOO??? https://t.co/u2Oq3NRGvR                  |      1 |       1 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Invite you to join @v@Jub Jang????'s streaming on #BIGOLIVE. Join now! https://t.co/2MVZ2tnlJg https://t.co/enF1N7ydbF                                    |      1 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Found a Transponder Snail!Candid shots of the Straw Hats on their new ship!https://t.co/lGcHiFpLmb #TreCru https://t.co/Ehk4w75cl2                        |      1 |       1 |              1 |          0 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @UItraSuristic: AS:ASENSIO OVERTAKES ISCO &amp; JAMESMarco played 184mins vs Sevilla &amp; SociedadIsco only 84.James least with 62. htt‚Ä¶              |      1 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @ShaiLinne: Feels so good to (finally) be back in the studio! So much to be said! #StillJesus https://t.co/zSz8cT8gIg                                  |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @TheRealMMyer: 68 Days Until Halloween üéÉüî™ https://t.co/AnrNOW6Ga5                                                                                    |      1 |       1 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @ChelseaFC: It's been a glorious day at Stamford Bridge! Four hours to go until tonight's game v Bristol Rovers... https://t.co/IAwKTZ‚Ä¶                |      2 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Update: I'm still in love with this https://t.co/UukWsWpFuN                                                                                               |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| \"Servicing Woody's since before you were born.\" https://t.co/HGhJ158Iys                                                                                   |      1 |       1 |              1 |          0 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| See our latest #Rolla, MO #job and click to apply: Systems Analyst II - https://t.co/pwmwWZsRK4 #medicaljobs https://t.co/VsP778FG9y                      |      1 |       1 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @StarCinema: Kathryn, Daniel share memorable experience in Barcelona SEE INTERVIEW HERE: https://t.co/Vub69JTdKL https://t.co/NntyQWs7To               |      2 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @M_Douglass64: Need to reunite. Left in taxi from Cornhill to Noel St W1. Yesterday Please RT https://t.co/732bpzhrb8                                  |      1 |       2 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Good night..  #ribbonsday https://t.co/0GbxcB9IQu                                                                                                         |      2 |       2 |              2 |          1 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Invite you to join FM‚ôö'PearPraeü¶Ñüç≠'s streaming on #BIGOLIVE &gt; ‡∏°‡∏≤‡πÅ‡∏õ‡∏õ‡∏ô‡∏∏‡∏á‡∏á. Join now!  https://t.co/g45uVdyAJp https://t.co/PuItZLP9iB                    |      1 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| NEW: emilia clarke with fans https://t.co/vSOGIuQ7Mg                                                                                                      |      1 |       1 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Happy birthday to the most handsome kid in the world, love u like the son I never had congrats on the L @Snavitsky7 https://t.co/wGGb6sXTUt               |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @NathanZed: Follow your roommate on Twitter https://t.co/oFzx4JuwMU                                                                                    |      1 |       1 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @namkachu: start from the bottom and now we're here https://t.co/md0EoFfV6U                                                                            |      1 |       1 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| Come here, let me put you on some music ???? https://t.co/JfJe4ha2gN                                                                                      |      1 |       1 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @SKDurrani_: @AatifAzio people of Pak are politically aware now, thanks to #ImranKhan https://t.co/LYHZ7ERsyd                                          |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| @loaded is CHARTED 1x @ Beatport. BIGUP to Holden &amp; Thompson README https://t.co/PcsGHNIXHG https://t.co/Lzo4o0Gvt1                                   |      1 |       1 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @abbieaustin3: honestly if I'm not livin this life in 20 years from now, idk where I'll beüåû https://t.co/VSoUx29d66                                   |      1 |       2 |              1 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @Ish_Bhandari: Every day, Media scums hit new depth. A medal winner for India &amp; they chose this headline https://t.co/NabDSk4Ehc                   |      1 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @giveasyoulive: Yes #CharityTuesday - Tweet us your chosen charity to WIN them a ¬£20 donation &amp; yourself a ¬£20 @amazon voucher #WOW http‚Ä¶          |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @AdrienneTiffith: Annoucument: Students please stand for the pledge allegiance *Black Students* https://t.co/QFK06pNV3f                                |      1 |       2 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| #Web https://t.co/Ydvnbjbkwy * NASCAR VIDEO * #Blog #WebSite business‚Ä¶ https://t.co/BmGfKRDu8Y #ForSale #WebSites https://t.co/4Up0nc3WgN                 |      1 |       1 |              1 |          2 |          2 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| #Repost of @champagnebanjee making us look GOOD. Look at that muscle butt in a #FTGRUNT jock! #InstaGrunt https://t.co/E9H3FRurCo                         |      2 |       2 |              2 |          2 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n",
            "| RT @kennanoel4: Notice a difference? ???? https://t.co/82axKAjZG9                                                                                         |      1 |       1 |              0 |          0 |          0 |       2 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+--------+---------+----------------+------------+------------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZdmYqIbt8DV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}