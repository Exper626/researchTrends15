{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303020a-a75b-4e21-971a-8ed8859f0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "\n",
    "class ModalityEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward subnet used for Audio and Video before fusion.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dimenstion, hidden_dimenstion, dropout_probability):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dimenstion)\n",
    "        self.dropout    = nn.Dropout(p=dropout_probability)\n",
    "        self.fc_layer1 = nn.Linear(input_dim, hidden_dimenstion)\n",
    "        self.fc_layer2 = nn.Linear(hidden_dim, hidden_dimenstion)\n",
    "        self.fc_layer3 = nn.Linear(hidden_dim, hidden_dimenstion)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        normalized = self.batch_norm(inputs)\n",
    "        dropped    = self.dropout(normalized)\n",
    "        hidden1    = F.relu(self.fc_layer1(dropped))\n",
    "        hidden2    = F.relu(self.fc_layer2(hidden1))\n",
    "        hidden3    = F.relu(self.fc_layer3(hidden2))\n",
    "        \n",
    "        return hidden3\n",
    "\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based Text processing subnet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.2, bidirectional=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text_sequence):\n",
    "        _, (hidden_states, _) = self.lstm(text_sequence)\n",
    "\n",
    "        last_hidden = hidden_states[-1]                     # last LSTM layer output\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "\n",
    "        output_features = self.output_layer(last_hidden)\n",
    "        return output_features\n",
    "\n",
    "\n",
    "\n",
    "class LMF(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-rank Multimodal Fusion (LMF)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dimenstion, hidden_dimenstion, text_output_dimenstion, dropouts, output_dimenstion, rank):\n",
    "\n",
    "        super().__init__()\n",
    "        audio_raw_input, video_raw_input, text_raw_input = input_dimenstion\n",
    "        audio_hidden, video_hidden, text_hidden = hidden_dimenstion\n",
    "\n",
    "        self.rank = rank\n",
    "        self.output_dimenstion = output_dimenstion\n",
    "        \n",
    "\n",
    "        self.audio_encoder = ModalityEncoder(audio_raw_input, audio_hidden, dropouts[0])\n",
    "        self.video_encoder = ModalityEncoder(video_raw_input, video_hidden, dropouts[1])\n",
    "        self.text_encoder  = TextEncoder    (text_raw_input, text_hidden, text_output_dim, dropout=dropouts[2])\n",
    "\n",
    "        self.post_fusion_dropout = nn.Dropout(p=dropouts[3])\n",
    "\n",
    "        bias_term = 1\n",
    "\n",
    "        self.audio_factor = nn.Parameter(\n",
    "            torch.Tensor(rank, audio_hidden + bias_term, output_dim)\n",
    "        )\n",
    "        self.video_factor = nn.Parameter(\n",
    "            torch.Tensor(rank, video_hidden + bias_term, output_dim)\n",
    "        )\n",
    "        self.text_factor = nn.Parameter(\n",
    "            torch.Tensor(rank, text_output_dim + bias_term, output_dim)\n",
    "        )\n",
    "\n",
    "        self.rank_weights = nn.Parameter(torch.Tensor(rank))\n",
    "        self.output_bias  = nn.Parameter(torch.zeros(1, output_dim))\n",
    "\n",
    "        xavier_normal_(self.audio_factor)\n",
    "        xavier_normal_(self.video_factor)\n",
    "        xavier_normal_(self.text_factor)\n",
    "        xavier_normal_(self.rank_weights.view(1, -1))\n",
    "    \n",
    "    def forward(self, audio_raw_input, video_raw_input, text_raw_input):\n",
    "        \n",
    "        batch_size = audio_input.size(0)\n",
    "        device     = audio_input.device\n",
    "        \n",
    "        audio_extracted_features = self.audio_subnet(audio_raw_input)\n",
    "        video_extracted_features = self.video_subnet(video_raw_input)\n",
    "        text_extracted_features  = self.text_subnet(text_raw_input)\n",
    "        \n",
    "        bias_column = torch.ones(batch_size, 1, device=device)\n",
    "        audio_with_bias = torch.cat([bias_column, audio_extracted_features], dim=1)\n",
    "        video_with_bias = torch.cat([bias_column, video_extracted_features], dim=1)\n",
    "        text_with_bias  = torch.cat([bias_column, text_extracted_features],  dim=1)\n",
    "        \n",
    "        audio_rank_projections = torch.einsum(\"bf, rfo -> bro\", audio_with_bias, self.audio_factor)\n",
    "        video_rank_projections = torch.einsum(\"bf, rfo -> bro\", video_with_bias, self.video_factor)\n",
    "        text_rank_projections  = torch.einsum(\"bf, rfo -> bro\", text_with_bias,  self.text_factor)\n",
    "        \n",
    "        fused_all_modalities_tensor = audio_rank_projections * video_rank_projections * text_rank_projections\n",
    "        fused_all_modalities_tensor = self.post_fusion_dropout(fused_all_modalities_tensor)\n",
    "        \n",
    "        logits = torch.einsum(\"bro, r -> bo\", all_modalities_fused_tensor, self.rank_weights)\n",
    "        logits = logits + self.output_bias\n",
    "        \n",
    "        normalized_probability = F.softmax(logits, dim=1)\n",
    "        return normalized_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d3dfe-e94a-4eac-9e87-13401ae30a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 32\n",
    "t = torch.randn(batch, 300)   # text features\n",
    "a = torch.randn(batch, 74)    # audio features\n",
    "v = torch.randn(batch, 35)    # video features\n",
    "\n",
    "model = LMF(300, 74, 35, output_dim=1, rank=8)\n",
    "out = model(t, a, v)\n",
    "\n",
    "print(out.shape)   # → torch.Size([32, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc512dfe-08e9-4762-92d9-ef56a6ada03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "\n",
    "class ModalityEncoder(nn.Module)\n",
    "\n",
    "    def __init__(self, input_dimenstion, hidden_dimenstion, dropout_probability):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dimenstion)\n",
    "        self.dropout    = nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        # FIX: use correct variable names\n",
    "        self.fc_layer1 = nn.Linear(input_dimenstion, hidden_dimenstion)\n",
    "        self.fc_layer2 = nn.Linear(hidden_dimenstion, hidden_dimenstion)\n",
    "        self.fc_layer3 = nn.Linear(hidden_dimenstion, hidden_dimenstion)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        normalized = self.batch_norm(inputs)\n",
    "        dropped    = self.dropout(normalized)\n",
    "        hidden1    = F.relu(self.fc_layer1(dropped))\n",
    "        hidden2    = F.relu(self.fc_layer2(hidden1))\n",
    "        hidden3    = F.relu(self.fc_layer3(hidden2))\n",
    "        return hidden3\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based Text processing subnet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text_sequence):\n",
    "        _, (hidden_states, _) = self.lstm(text_sequence)\n",
    "\n",
    "        last_hidden = hidden_states[-1]              # last layer output\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "\n",
    "        output_features = self.output_layer(last_hidden)\n",
    "        return output_features\n",
    "\n",
    "\n",
    "class LMF(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-rank Multimodal Fusion (LMF)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dimenstion, hidden_dimenstion, text_output_dimenstion, dropouts, output_dimenstion, rank):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Unpack inputs\n",
    "        audio_raw_input, video_raw_input, text_raw_input = input_dimenstion\n",
    "        audio_hidden, video_hidden, text_hidden = hidden_dimenstion\n",
    "\n",
    "        self.rank = rank\n",
    "        self.output_dimenstion = output_dimenstion\n",
    "\n",
    "        # BUILD ENCODERS (Fixed names)\n",
    "        self.audio_subnet = ModalityEncoder(audio_raw_input, audio_hidden, dropouts[0])\n",
    "        self.video_subnet = ModalityEncoder(video_raw_input, video_hidden, dropouts[1])\n",
    "        self.text_subnet  = TextEncoder(text_raw_input, text_hidden, text_output_dimenstion, dropout=dropouts[2])\n",
    "\n",
    "        self.post_fusion_dropout = nn.Dropout(p=dropouts[3])\n",
    "\n",
    "        bias_term = 1\n",
    "\n",
    "        # Low-rank factors — correct shapes\n",
    "        self.audio_factor = nn.Parameter(\n",
    "            torch.Tensor(rank, audio_hidden + bias_term, output_dimenstion)\n",
    "        )\n",
    "        self.video_factor = nn.Parameter(\n",
    "            torch.Tensor(rank, video_hidden + bias_term, output_dimenstion)\n",
    "        )\n",
    "        self.text_factor = nn.Parameter(\n",
    "            torch.Tensor(rank, text_output_dimenstion + bias_term, output_dimenstion)\n",
    "        )\n",
    "\n",
    "        self.rank_weights = nn.Parameter(torch.Tensor(rank))\n",
    "        self.output_bias  = nn.Parameter(torch.zeros(1, output_dimenstion))\n",
    "\n",
    "        # Initialize\n",
    "        xavier_normal_(self.audio_factor)\n",
    "        xavier_normal_(self.video_factor)\n",
    "        xavier_normal_(self.text_factor)\n",
    "        xavier_normal_(self.rank_weights.view(1, -1))\n",
    "\n",
    "\n",
    "    def forward(self, audio_raw_input, video_raw_input, text_raw_input):\n",
    "\n",
    "        batch_size = audio_raw_input.size(0)\n",
    "        device     = audio_raw_input.device\n",
    "\n",
    "        # 1. Extract Features\n",
    "        audio_extracted_features = self.audio_subnet(audio_raw_input)\n",
    "        video_extracted_features = self.video_subnet(video_raw_input)\n",
    "        text_extracted_features  = self.text_subnet(text_raw_input)\n",
    "\n",
    "        # 2. Add Bias Term\n",
    "        bias_column = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "        audio_with_bias = torch.cat([bias_column, audio_extracted_features], dim=1)\n",
    "        video_with_bias = torch.cat([bias_column, video_extracted_features], dim=1)\n",
    "        text_with_bias  = torch.cat([bias_column, text_extracted_features],  dim=1)\n",
    "\n",
    "        # 3. Apply rank projections (einsum fully correct)\n",
    "        audio_rank_projections = torch.einsum(\"bf, rfo -> bro\",\n",
    "                                              audio_with_bias,\n",
    "                                              self.audio_factor)\n",
    "        video_rank_projections = torch.einsum(\"bf, rfo -> bro\",\n",
    "                                              video_with_bias,\n",
    "                                              self.video_factor)\n",
    "        text_rank_projections  = torch.einsum(\"bf, rfo -> bro\",\n",
    "                                              text_with_bias,\n",
    "                                              self.text_factor)\n",
    "\n",
    "        # 4. Element-wise product\n",
    "        fused_all_modalities_tensor = (\n",
    "            audio_rank_projections *\n",
    "            video_rank_projections *\n",
    "            text_rank_projections\n",
    "        )\n",
    "\n",
    "        fused_all_modalities_tensor = self.post_fusion_dropout(fused_all_modalities_tensor)\n",
    "\n",
    "        # 5. Combine ranks → final fused vector\n",
    "        logits = torch.einsum(\"bro, r -> bo\",\n",
    "                              fused_all_modalities_tensor,\n",
    "                              self.rank_weights)\n",
    "\n",
    "        logits = logits + self.output_bias\n",
    "\n",
    "        return logits      # DO NOT softmax here (classifier will handle it)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
