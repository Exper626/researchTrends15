{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45298314-d9c3-4398-a650-1b70a6686079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Unimodal_Feature_Extraction_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, k):\n",
    "        super(UFEN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(x)\n",
    "        \n",
    "        avg_pool = torch.mean(output, dim=1)\n",
    "        projected = self.fc(avg_pool)\n",
    "        return projected # Shape: (batch, k)\n",
    "\n",
    "class ABSLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    RIA - Recurrent Interaction Attention\n",
    "    GIA - Global Interation Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, attention_window_t_RIA):\n",
    "        super(ABSLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.t = attention_window_t_RIA # Window size for RIA\n",
    "      \n",
    "        self.W_f1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.W_f2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        self.W_i1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.W_i2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        self.W_m1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.W_m2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        self.W_o1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.W_o2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        self.W_c = nn.Linear(hidden_dim + input_dim, hidden_dim) \n",
    "        self.W_h = nn.Linear(hidden_dim + input_dim, hidden_dim)\n",
    "\n",
    "        self.W_h_prime = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_x_prime = nn.Linear(input_dim, hidden_dim) \n",
    "        \n",
    "        self.W_h2 = nn.Linear(hidden_dim, 1, bias=False) \n",
    "        self.W_x2 = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x_f, h_prev, c_prev, h_history, c_history):\n",
    "        \"\"\"\n",
    "        x_f: Current input chunk (Batch, Input_Dim)\n",
    "        h_prev, c_prev: Previous step hidden/cell (Batch, Hidden_Dim)\n",
    "        h_history, c_history: Lists of previous tensors for RIA\n",
    "        \"\"\"\n",
    "        batch_size = x_f.size(0)\n",
    "        \n",
    "        history_len = len(h_history)\n",
    "        if history_len == 0:\n",
    "            # If no history, context is just previous state\n",
    "            c_tilde = c_prev\n",
    "            h_tilde = h_prev\n",
    "        else:\n",
    "            # Calculate scores for history\n",
    "            # We iterate through history items to compute attention scores\n",
    "            s_c_list = []\n",
    "            s_h_list = []\n",
    "            \n",
    "            # Using recent history up to self.t\n",
    "            start_idx = max(0, history_len - self.t)\n",
    "            relevant_h = h_history[start_idx:]\n",
    "            relevant_c = c_history[start_idx:]\n",
    "            \n",
    "            for h_past, c_past in zip(relevant_h, relevant_c):\n",
    "                # Eq 5: Concat past state with CURRENT input x_f\n",
    "                cat_c = torch.cat([c_past, x_f], dim=1)\n",
    "                cat_h = torch.cat([h_past, x_f], dim=1)\n",
    "                \n",
    "                # Compute score vectors (tanh activation per Eq 5)\n",
    "                s_c_vec = torch.tanh(self.W_c(cat_c))\n",
    "                s_h_vec = torch.tanh(self.W_h(cat_h))\n",
    "                \n",
    "                # Eq 6: L2 norm to get scalar score per history item\n",
    "                s_c_list.append(torch.norm(s_c_vec, p=2, dim=1, keepdim=True))\n",
    "                s_h_list.append(torch.norm(s_h_vec, p=2, dim=1, keepdim=True))\n",
    "            \n",
    "            # Stack scores: (Batch, History_Len)\n",
    "            s_c_stack = torch.cat(s_c_list, dim=1)\n",
    "            s_h_stack = torch.cat(s_h_list, dim=1)\n",
    "            \n",
    "            # Eq 7: Softmax\n",
    "            gamma_c = F.softmax(s_c_stack, dim=1)\n",
    "            gamma_h = F.softmax(s_h_stack, dim=1)\n",
    "            \n",
    "            # Eq 8: Weighted Sum\n",
    "            c_tilde_sum = torch.zeros_like(c_prev)\n",
    "            h_tilde_sum = torch.zeros_like(h_prev)\n",
    "            \n",
    "            for idx, (h_past, c_past) in enumerate(zip(relevant_h, relevant_c)):\n",
    "                w_c = gamma_c[:, idx].unsqueeze(1)\n",
    "                w_h = gamma_h[:, idx].unsqueeze(1)\n",
    "                c_tilde_sum += w_c * c_past\n",
    "                h_tilde_sum += w_h * h_past\n",
    "                \n",
    "            c_tilde = F.relu(c_tilde_sum) # 'a' is ReLU per text\n",
    "            h_tilde = F.relu(h_tilde_sum)\n",
    "\n",
    "        # --- 2. Standard LSTM Gates Updates ---\n",
    "        # Eq 9, 10\n",
    "        f_l = torch.sigmoid(self.W_f1(x_f) + self.W_f2(h_tilde))\n",
    "        i_l = torch.sigmoid(self.W_i1(x_f) + self.W_i2(h_tilde))\n",
    "        \n",
    "        # Eq 11: Candidate Cell\n",
    "        c_candidate = torch.tanh(self.W_m1(x_f) + self.W_m2(h_tilde))\n",
    "        \n",
    "        # Update Cell State\n",
    "        c_l = f_l * c_tilde + i_l * c_candidate\n",
    "\n",
    "        # Output Gate (Eq 12)\n",
    "        o_l = torch.sigmoid(self.W_o1(x_f) + self.W_o2(h_tilde))\n",
    "        # Initial hidden state (before GIA)\n",
    "        h_l_initial = o_l * torch.tanh(c_l)\n",
    "\n",
    "        # --- 3. Global Interaction Attention (GIA) ---\n",
    "        # Eq 13: Omega_h\n",
    "        omega_h = F.relu(self.W_h_prime(h_l_initial))\n",
    "        # Eq 14: Omega_x\n",
    "        omega_x = F.relu(self.W_x_prime(x_f))\n",
    "        \n",
    "        # Eq 15: Attended State Calculation\n",
    "        # The text says: omega_x is pre-multiplied by W_x2 and added as BIAS\n",
    "        # The text says: W_h2 and omega_h form a scalar weight for h_l\n",
    "        \n",
    "        # Scalar weight for hidden state: (Batch, 1)\n",
    "        weight_h = torch.matmul(omega_h, self.W_h2.weight.t()) \n",
    "        # Scalar bias from input: (Batch, 1)\n",
    "        bias_x = torch.matmul(omega_x, self.W_x2.weight.t())\n",
    "        \n",
    "        # Apply attention\n",
    "        # Note: Dimensions in text are tricky. W_h2 is 1xO. \n",
    "        # We implement it as element-wise scaling or scalar scaling based on text description.\n",
    "        # \"W_h2 and omega_h first form a scalar... which reflects importance\"\n",
    "        # \"pre-multiply omega_x by W_x2 and obtain a scalar... added as bias\"\n",
    "        \n",
    "        h_l_attended = torch.tanh(weight_h * h_l_initial + bias_x)\n",
    "        \n",
    "        return h_l_attended, c_l\n",
    "\n",
    "class HFFN(nn.Module):\n",
    "    def __init__(self, k, d, s, hidden_dim_o, num_classes):\n",
    "        super(HFFN, self).__init__()\n",
    "        self.k = k # Feature length\n",
    "        self.d = d # Window size\n",
    "        self.s = s # Stride\n",
    "        self.hidden_dim = hidden_dim_o\n",
    "        \n",
    "        # Divide Stage: Calculate number of chunks\n",
    "        # n = (k - d) // s + 2 (because of padding logic in text)\n",
    "        # We will handle padding dynamically in forward\n",
    "        \n",
    "        # Conquer Stage: Outer Product Dimension\n",
    "        # Each vector is padded with 1, so size is d+1\n",
    "        # Tensor size is (d+1)^3\n",
    "        self.fusion_dim = (d + 1) ** 3\n",
    "        \n",
    "        # Combine Stage: ABS-LSTM\n",
    "        self.abs_lstm_cell = ABSLSTMCell(self.fusion_dim, hidden_dim_o, attention_window_t=5)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Inference Stage\n",
    "        # Input is concatenation of all hidden states (Bidirectional)\n",
    "        # Output of ABS-LSTM is sequence of size 2*o\n",
    "        # Text says \"f contains a tanh... and dropout... We1 in R^{50 x n*2o}\"\n",
    "        # It seems they flatten the WHOLE sequence of states for the final classifier?\n",
    "        # \"I = softmax(We2 E)... where We1... E = f(We1 X^g + b)\"\n",
    "        # X^g is R^{n x 2o}. If We1 is 50 x (n*2o), they flatten X^g.\n",
    "        \n",
    "        # We defer the definition of the linear layer until we know 'n' (runtime)\n",
    "        # or we define it based on max 'n'.\n",
    "        self.inference_fc1 = None # Will init in forward or need fixed k\n",
    "        self.inference_fc2 = nn.Linear(50, num_classes)\n",
    "        \n",
    "    def _get_local_chunks(self, vec, modality_name):\n",
    "        \"\"\"\n",
    "        Implements Eq 1: Sliding Window\n",
    "        Input: vec (Batch, k)\n",
    "        Output: (Batch, n, d)\n",
    "        \"\"\"\n",
    "        batch_size = vec.size(0)\n",
    "        # Pad with zeros if necessary to make divisible\n",
    "        # Text says: \"feature vectors are padded with 0s to guarantee divisibility\"\n",
    "        # The logic: n = floor((k-d)/s) + 2\n",
    "        # We use PyTorch unfold.\n",
    "        \n",
    "        # Reshape to (Batch, 1, 1, k) for unfold, or just use simple loop\n",
    "        # Unfold on dimension 1\n",
    "        # Input to unfold: (Batch, Channel, Length) -> (Batch, 1, k)\n",
    "        x = vec.unsqueeze(1)\n",
    "        \n",
    "        # We need to pad 'x' to ensure we cover everything + the extra pad mentioned\n",
    "        # Simple approach: Manual slicing to match Eq 1 exactly\n",
    "        chunks = []\n",
    "        i = 1\n",
    "        while True:\n",
    "            start_idx = self.s * (i - 1)\n",
    "            end_idx = start_idx + self.d\n",
    "            \n",
    "            if end_idx > self.k:\n",
    "                # Padding case (Last chunk)\n",
    "                # Text says \"padded with 0s\"\n",
    "                remainder = vec[:, start_idx:]\n",
    "                pad_len = self.d - remainder.size(1)\n",
    "                if pad_len > 0:\n",
    "                    padding = torch.zeros(batch_size, pad_len).to(vec.device)\n",
    "                    chunk = torch.cat([remainder, padding], dim=1)\n",
    "                    chunks.append(chunk)\n",
    "                break\n",
    "            else:\n",
    "                chunk = vec[:, start_idx:end_idx]\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "        return torch.stack(chunks, dim=1) # (Batch, n, d)\n",
    "\n",
    "    def forward(self, l_vec, v_vec, a_vec):\n",
    "        \"\"\"\n",
    "        l_vec, v_vec, a_vec: (Batch, k)\n",
    "        \"\"\"\n",
    "        batch_size = l_vec.size(0)\n",
    "        \n",
    "        # --- 1. Divide Stage (Sliding Window) ---\n",
    "        l_chunks = self._get_local_chunks(l_vec, 'l') # (Batch, n, d)\n",
    "        v_chunks = self._get_local_chunks(v_vec, 'v')\n",
    "        a_chunks = self._get_local_chunks(a_vec, 'a')\n",
    "        \n",
    "        n = l_chunks.size(1) # Number of chunks\n",
    "        \n",
    "        # --- 2. Conquer Stage (Tensor Fusion) ---\n",
    "        # Eq 2: Pad with 1\n",
    "        ones = torch.ones(batch_size, n, 1).to(l_vec.device)\n",
    "        l_pad = torch.cat([l_chunks, ones], dim=2) # (Batch, n, d+1)\n",
    "        v_pad = torch.cat([v_chunks, ones], dim=2)\n",
    "        a_pad = torch.cat([a_chunks, ones], dim=2)\n",
    "        \n",
    "        # Eq 3: Outer Product\n",
    "        # We process each chunk.\n",
    "        # Result shape: (Batch, n, (d+1)^3)\n",
    "        # Einstein summation is easiest: b=batch, n=seq, i,j,k = dims\n",
    "        fused_seq = []\n",
    "        for t in range(n):\n",
    "            l_t = l_pad[:, t, :]\n",
    "            v_t = v_pad[:, t, :]\n",
    "            a_t = a_pad[:, t, :]\n",
    "            \n",
    "            # Outer product l (x) v (x) a\n",
    "            # (Batch, d+1) -> (Batch, d+1, d+1, d+1)\n",
    "            # using einsum: bi, bj, bk -> bijk\n",
    "            out_prod = torch.einsum('bi,bj,bk->bijk', l_t, v_t, a_t)\n",
    "            \n",
    "            # Flatten to vector\n",
    "            flat = out_prod.reshape(batch_size, -1) # (Batch, (d+1)^3)\n",
    "            fused_seq.append(flat)\n",
    "            \n",
    "        X_f = torch.stack(fused_seq, dim=1) # (Batch, n, input_dim)\n",
    "        \n",
    "        # --- 3. Combine Stage (ABS-LSTM) ---\n",
    "        # Bidirectional: Forward pass and Backward pass\n",
    "        \n",
    "        # -- Forward Pass --\n",
    "        h_fwd_list = []\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(l_vec.device)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(l_vec.device)\n",
    "        h_history = []\n",
    "        c_history = []\n",
    "        \n",
    "        for t in range(n):\n",
    "            x_curr = X_f[:, t, :]\n",
    "            h_curr, c_curr = self.abs_lstm_cell(x_curr, h_prev, c_prev, h_history, c_history)\n",
    "            \n",
    "            h_fwd_list.append(h_curr)\n",
    "            \n",
    "            # Update history (detach to prevent massive graph retention if needed, \n",
    "            # though standard BPTT keeps it. We keep it attached for gradients)\n",
    "            h_history.append(h_curr)\n",
    "            c_history.append(c_curr)\n",
    "            h_prev, c_prev = h_curr, c_curr\n",
    "            \n",
    "        # -- Backward Pass --\n",
    "        # Reverse input\n",
    "        X_f_rev = torch.flip(X_f, [1])\n",
    "        h_bwd_list = []\n",
    "        c_prev = torch.zeros(batch_size, self.hidden_dim).to(l_vec.device)\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_dim).to(l_vec.device)\n",
    "        h_history = []\n",
    "        c_history = []\n",
    "        \n",
    "        for t in range(n):\n",
    "            x_curr = X_f_rev[:, t, :]\n",
    "            h_curr, c_curr = self.abs_lstm_cell(x_curr, h_prev, c_prev, h_history, c_history)\n",
    "            h_bwd_list.append(h_curr)\n",
    "            h_history.append(h_curr)\n",
    "            c_history.append(c_curr)\n",
    "            h_prev, c_prev = h_curr, c_curr\n",
    "            \n",
    "        # Reverse backward results to match time order\n",
    "        h_bwd_list.reverse()\n",
    "        \n",
    "        # Concatenate: X^g = [h_fwd; h_bwd] per step\n",
    "        # Shape: (Batch, n, 2*o)\n",
    "        X_g_list = [torch.cat([f, b], dim=1) for f, b in zip(h_fwd_list, h_bwd_list)]\n",
    "        X_g = torch.cat(X_g_list, dim=1) # Concatenate along feature dimension? \n",
    "        # Wait, Eq 4 says X^g in R^{n x 2o}. \n",
    "        # The inference Eq 17 uses W_e1 in R^{50 x n*2o}.\n",
    "        # This implies we concatenate ALL steps into one giant vector.\n",
    "        X_g_flat = torch.cat(X_g_list, dim=1) # (Batch, n * 2 * o)\n",
    "        \n",
    "        # --- 4. Inference Module ---\n",
    "        if self.inference_fc1 is None:\n",
    "            # Initialize dynamically on first run based on flattened size\n",
    "            flattened_dim = X_g_flat.size(1)\n",
    "            self.inference_fc1 = nn.Linear(flattened_dim, 50).to(l_vec.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
