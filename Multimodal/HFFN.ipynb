{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f51ac-cb75-420d-91a2-d1e637f33852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LocalFusionModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, window_size, stride):\n",
    "        super(LocalFusionModule, self).__init__()\n",
    "        self.d = window_size\n",
    "        self.s = stride\n",
    "\n",
    "    def forward(self, l, v, a):\n",
    "        batch_size, k = l.shape\n",
    "        \n",
    "        # --- Divide (Sliding Window) ---\n",
    "        # Handle padding if k-d is not divisible by s\n",
    "        if (k - self.d) % self.s != 0:\n",
    "            pad_len = self.s - ((k - self.d) % self.s)\n",
    "            l = F.pad(l, (0, pad_len))\n",
    "            v = F.pad(v, (0, pad_len))\n",
    "            a = F.pad(a, (0, pad_len))\n",
    "        \n",
    "        # Create windows: (Batch, Num_Chunks, Window_Size)\n",
    "        # unfold dimension 1 (time/feature dim)\n",
    "        l_chunks = l.unfold(1, self.d, self.s)\n",
    "        v_chunks = v.unfold(1, self.d, self.s)\n",
    "        a_chunks = a.unfold(1, self.d, self.s)\n",
    "\n",
    "        # --- Conquer (Fusion) ---\n",
    "        # Pad local portion with 1s -> Shape (Batch, n, d+1)\n",
    "        ones = torch.ones(batch_size, l_chunks.size(1), 1).to(l.device)\n",
    "        l_pad = torch.cat([l_chunks, ones], dim=2)\n",
    "        v_pad = torch.cat([v_chunks, ones], dim=2)\n",
    "        a_pad = torch.cat([a_chunks, ones], dim=2)\n",
    "        \n",
    "        # Outer Product: (d+1) x (d+1) x (d+1)\n",
    "        # Output: (Batch, n, d+1, d+1, d+1)\n",
    "        fusion_tensor = torch.einsum('bni,bnj,bnk->bnijk', l_pad, v_pad, a_pad)\n",
    "        \n",
    "        # Flatten feature dims: (Batch, n, (d+1)^3)\n",
    "        X_f = fusion_tensor.reshape(batch_size, l_chunks.size(1), -1)\n",
    "        \n",
    "        return X_f\n",
    "\n",
    "\n",
    "class ABSLSTMCell(nn.Module):\n",
    "  \n",
    "    def __init__(self, input_dim, hidden_dim, lookback_t=3):\n",
    "        super(ABSLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.t = lookback_t\n",
    "\n",
    "        self.W_c = nn.Linear(hidden_dim + input_dim, hidden_dim, bias=True)\n",
    "        self.W_h = nn.Linear(hidden_dim + input_dim, hidden_dim, bias=True)\n",
    "        self.act_a = nn.ReLU() \n",
    "       \n",
    "        self.W_f1 = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.W_f2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_i1 = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.W_i2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_m1 = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.W_m2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_o1 = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.W_o2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x_curr, past_h_list, past_c_list):\n",
    "        # 1. RIA (Look-back Attention)\n",
    "        s_c_scores, s_h_scores = [], []\n",
    "        \n",
    "        for i in range(len(past_h_list)):\n",
    "            cat_c = torch.cat([past_c_list[i], x_curr], dim=1)\n",
    "            cat_h = torch.cat([past_h_list[i], x_curr], dim=1)\n",
    "            \n",
    "            s_c_vec = torch.tanh(self.W_c(cat_c))\n",
    "            s_h_vec = torch.tanh(self.W_h(cat_h))\n",
    "            \n",
    "            s_c_scores.append(torch.norm(s_c_vec, p=2, dim=1, keepdim=True))\n",
    "            s_h_scores.append(torch.norm(s_h_vec, p=2, dim=1, keepdim=True))\n",
    "            \n",
    "        # Softmax over time dimension (Eq 7)\n",
    "        gamma_c = F.softmax(torch.cat(s_c_scores, dim=1), dim=1).unsqueeze(2)\n",
    "        gamma_h = F.softmax(torch.cat(s_h_scores, dim=1), dim=1).unsqueeze(2)\n",
    "        \n",
    "        # Weighted sum of past states (Eq 8)\n",
    "        c_tilde = torch.sum(gamma_c * torch.stack(past_c_list, dim=1), dim=1)\n",
    "        h_tilde = torch.sum(gamma_h * torch.stack(past_h_list, dim=1), dim=1)\n",
    "        \n",
    "        # Apply activation 'a'\n",
    "        c_tilde = self.act_a(c_tilde)\n",
    "        h_tilde = self.act_a(h_tilde)\n",
    "        \n",
    "        # 2. LSTM Logic (Eq 9-12)\n",
    "        f_l = torch.sigmoid(self.W_f1(x_curr) + self.W_f2(h_tilde))\n",
    "        i_l = torch.sigmoid(self.W_i1(x_curr) + self.W_i2(h_tilde))\n",
    "        m_l = torch.tanh(self.W_m1(x_curr) + self.W_m2(h_tilde))\n",
    "        o_l = torch.sigmoid(self.W_o1(x_curr) + self.W_o2(h_tilde))\n",
    "        \n",
    "        c_l = f_l * c_tilde + i_l * m_l\n",
    "        h_l = o_l * torch.tanh(c_l)\n",
    "        \n",
    "        return h_l, c_l\n",
    "\n",
    "class GlobalFusionModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_o, lookback_t=3):\n",
    "        \"\"\"\n",
    "        Bidirectional ABS-LSTM with Global Interaction Attention (GIA).\n",
    "        \"\"\"\n",
    "        super(GlobalFusionModule, self).__init__()\n",
    "        self.hidden_dim = hidden_dim_o\n",
    "        self.t = lookback_t\n",
    "        \n",
    "        # Shared Cell for both directions\n",
    "        self.abs_lstm_cell = ABSLSTMCell(input_dim, hidden_dim_o, lookback_t)\n",
    "        \n",
    "        # GIA Weights (Eq 13-15)\n",
    "        self.bi_hidden_dim = 2 * hidden_dim_o\n",
    "        self.W_h_prime = nn.Linear(self.bi_hidden_dim, hidden_dim_o, bias=True)\n",
    "        self.W_x = nn.Linear(input_dim, hidden_dim_o, bias=True)\n",
    "        self.W_h2 = nn.Linear(hidden_dim_o, 1, bias=False)\n",
    "        self.W_x2 = nn.Linear(hidden_dim_o, 1, bias=False)\n",
    "\n",
    "    def _run_direction(self, x_seq):\n",
    "        batch_size, seq_len, _ = x_seq.size()\n",
    "        \n",
    "        # Init buffers for 't' past steps\n",
    "        past_h = [torch.zeros(batch_size, self.hidden_dim).to(x_seq.device) for _ in range(self.t)]\n",
    "        past_c = [torch.zeros(batch_size, self.hidden_dim).to(x_seq.device) for _ in range(self.t)]\n",
    "        \n",
    "        h_outputs = []\n",
    "        for l in range(seq_len):\n",
    "            x_curr = x_seq[:, l, :]\n",
    "            h_l, c_l = self.abs_lstm_cell(x_curr, past_h, past_c)\n",
    "            h_outputs.append(h_l)\n",
    "            \n",
    "            # Slide window (pop old, push new)\n",
    "            past_h.pop(0); past_h.append(h_l)\n",
    "            past_c.pop(0); past_c.append(c_l)\n",
    "            \n",
    "        return torch.stack(h_outputs, dim=1)\n",
    "\n",
    "    def forward(self, x_f):\n",
    "        # Bidirectional Pass\n",
    "        h_fwd = self._run_direction(x_f)\n",
    "        \n",
    "        x_rev = torch.flip(x_f, dims=[1])\n",
    "        h_bwd = self._run_direction(x_rev)\n",
    "        h_bwd = torch.flip(h_bwd, dims=[1]) # Restore order\n",
    "        \n",
    "        # Concatenate: (Batch, n, 2o)\n",
    "        h_l = torch.cat([h_bwd, h_fwd], dim=2)\n",
    "        \n",
    "        # Global Interaction Attention (GIA)\n",
    "        omega_h = F.relu(self.W_h_prime(h_l))\n",
    "        omega_x = F.relu(self.W_x(x_f))\n",
    "        \n",
    "        scalar_h = self.W_h2(omega_h)\n",
    "        scalar_x = self.W_x2(omega_x)\n",
    "        \n",
    "        # Apply GIA weights (Eq 15)\n",
    "        h_l_att = torch.tanh(scalar_h * h_l + scalar_x)\n",
    "        \n",
    "        return h_l_att\n",
    "\n",
    "\n",
    "class EmotionInferenceModule(nn.Module):\n",
    "    def __init__(self, num_chunks, hidden_dim_o, num_classes, intermediate_dim=50):\n",
    "        super(EmotionInferenceModule, self).__init__()\n",
    "        \n",
    "        # Input size: n * 2o (all timesteps concatenated)\n",
    "        self.input_dim = num_chunks * (2 * hidden_dim_o)\n",
    "        \n",
    "        self.W_e1 = nn.Linear(self.input_dim, intermediate_dim, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.W_e2 = nn.Linear(intermediate_dim, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x_g):\n",
    "        # Flatten: (Batch, n * 2o)\n",
    "        x_flat = x_g.view(x_g.size(0), -1)\n",
    "        \n",
    "        # Eq 16\n",
    "        e = torch.tanh(self.W_e1(x_flat))\n",
    "        e = self.dropout(e)\n",
    "        \n",
    "        # Eq 17\n",
    "        logits = self.W_e2(e)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "class HFFN(nn.Module):\n",
    "    def __init__(self, k, d, s, o, num_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k: Feature vector length per modality.\n",
    "            d: Window size.\n",
    "            s: Stride.\n",
    "            o: LSTM hidden dim.\n",
    "            num_classes: Output classes.\n",
    "        \"\"\"\n",
    "        super(HFFN, self).__init__()\n",
    "        \n",
    "        # Calculate expected number of chunks (n)\n",
    "        # Since LFM pads dynamically, we calculate n based on the padded length logic.\n",
    "        # k - d must be divisible by s.\n",
    "        remainder = (k - d) % s\n",
    "        if remainder == 0:\n",
    "            pad = 0\n",
    "        else:\n",
    "            pad = s - remainder\n",
    "            \n",
    "        k_padded = k + pad\n",
    "        self.n = ((k_padded - d) // s) + 1\n",
    "        \n",
    "        self.lfm = LocalFusionModule(d, s)\n",
    "        \n",
    "        # GFM input dim = (d+1)^3\n",
    "        lfm_out_dim = (d + 1) ** 3\n",
    "        self.gfm = GlobalFusionModule(lfm_out_dim, o, lookback_t=3)\n",
    "        \n",
    "        self.eim = EmotionInferenceModule(self.n, o, num_classes)\n",
    "\n",
    "    def forward(self, l, v, a):\n",
    "        x_f = self.lfm(l, v, a)\n",
    "        x_g = self.gfm(x_f)\n",
    "        output = self.eim(x_g)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db494e5-6177-47df-aced-cd154ce20c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    BATCH_SIZE = 8\n",
    "    K_LEN = 50       # Feature length (k)\n",
    "    D_WIN = 5        # Window size (d)\n",
    "    S_STRIDE = 2     # Stride (s)\n",
    "    O_HIDDEN = 32    # LSTM Hidden (o)\n",
    "    N_CLASSES = 6    # Emotion classes\n",
    "    \n",
    "    # --- Instantiate Model ---\n",
    "    model = HFFN(K_LEN, D_WIN, S_STRIDE, O_HIDDEN, N_CLASSES)\n",
    "    print(\"Model initialized successfully.\")\n",
    "    \n",
    "    # --- Dummy Input Data ---\n",
    "    # Random Tensors for Language, Visual, Acoustic\n",
    "    l_in = torch.randn(BATCH_SIZE, K_LEN)\n",
    "    v_in = torch.randn(BATCH_SIZE, K_LEN)\n",
    "    a_in = torch.randn(BATCH_SIZE, K_LEN)\n",
    "    \n",
    "    # --- Forward Pass ---\n",
    "    try:\n",
    "        preds = model(l_in, v_in, a_in)\n",
    "        print(\"\\n--- Forward Pass Successful ---\")\n",
    "        print(f\"Input Shape (per modality): {(BATCH_SIZE, K_LEN)}\")\n",
    "        print(f\"Output Shape (Probabilities): {preds.shape}\")\n",
    "        print(f\"Sample Prediction (First Item): \\n{preds[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during forward pass: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
