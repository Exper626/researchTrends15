{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45298314-d9c3-4398-a650-1b70a6686079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleEncoder(nn.Module):\n",
    "    \"\"\"Small MLP encoder used for each modality. Replace with CNN/RNN/Transformer as needed.\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, features)\n",
    "        return self.net(x)  # (batch, out_dim)\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"Cross-attention block between two feature sets A and B.\n",
    "    We compute attention from A -> B and return updated A and B (residual style).\n",
    "    This block is symmetric and lightweight (linear projections + scaled dot-product).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        # projections for queries/keys/values for both directions\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        # x: (batch, seq_len, dim) or (batch, dim) -> treat as seq_len=1\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        b, s, d = x.shape\n",
    "        x = x.view(b, s, self.num_heads, self.head_dim).transpose(1, 2)  # (b, heads, s, head_dim)\n",
    "        return x\n",
    "\n",
    "    def _merge_heads(self, x):\n",
    "        # x: (b, heads, s, head_dim)\n",
    "        x = x.transpose(1, 2).contiguous()  # (b, s, heads, head_dim)\n",
    "        b, s, h, hd = x.shape\n",
    "        return x.view(b, s, h * hd)\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        \"\"\"A, B: tensors of shape (batch, dim) or (batch, seq_len, dim)\n",
    "        Returns updated (A', B') of the same shapes.\n",
    "        \"\"\"\n",
    "        # normalize inputs\n",
    "        A_ln = self.norm(A)\n",
    "        B_ln = self.norm(B)\n",
    "\n",
    "        # project\n",
    "        qA = self.q_proj(A_ln)\n",
    "        kB = self.k_proj(B_ln)\n",
    "        vB = self.v_proj(B_ln)\n",
    "\n",
    "        qB = self.q_proj(B_ln)\n",
    "        kA = self.k_proj(A_ln)\n",
    "        vA = self.v_proj(A_ln)\n",
    "\n",
    "        # split heads\n",
    "        qA_h = self._split_heads(qA)\n",
    "        kB_h = self._split_heads(kB)\n",
    "        vB_h = self._split_heads(vB)\n",
    "\n",
    "        qB_h = self._split_heads(qB)\n",
    "        kA_h = self._split_heads(kA)\n",
    "        vA_h = self._split_heads(vA)\n",
    "\n",
    "        # scaled dot-product attention A->B\n",
    "        scale = (self.head_dim) ** -0.5\n",
    "        attn_logits = torch.matmul(qA_h, kB_h.transpose(-2, -1)) * scale  # (b, heads, sA, sB)\n",
    "        attn = torch.softmax(attn_logits, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        attendedB = torch.matmul(attn, vB_h)  # (b, heads, sA, head_dim)\n",
    "        attendedB = self._merge_heads(attendedB)  # (b, sA, dim)\n",
    "\n",
    "        # scaled dot-product attention B->A\n",
    "        attn_logits2 = torch.matmul(qB_h, kA_h.transpose(-2, -1)) * scale\n",
    "        attn2 = torch.softmax(attn_logits2, dim=-1)\n",
    "        attn2 = self.dropout(attn2)\n",
    "        attendedA = torch.matmul(attn2, vA_h)\n",
    "        attendedA = self._merge_heads(attendedA)\n",
    "\n",
    "        # linear + residual\n",
    "        outA = self.out(attendedB)\n",
    "        outB = self.out(attendedA)\n",
    "\n",
    "        # If original inputs were 2D (batch, dim), squeeze\n",
    "        if A.dim() == 2:\n",
    "            outA = outA.squeeze(1)\n",
    "            outB = outB.squeeze(1)\n",
    "\n",
    "        A_up = A + self.dropout(outA)\n",
    "        B_up = B + self.dropout(outB)\n",
    "\n",
    "        return A_up, B_up\n",
    "\n",
    "\n",
    "class HierarchicalFeatureFusionNetwork(nn.Module):\n",
    "    \"\"\"A compact HFFN example supporting N modalities.\n",
    "\n",
    "    Design:\n",
    "      1) Per-modality encoder -> produce modality embeddings (batch, dim)\n",
    "      2) Early fusion: pairwise concatenation/project to shared dim\n",
    "      3) Mid-level: hierarchical cross-attention between modality pairs (multiple stages)\n",
    "      4) Pooling + late fusion classifier\n",
    "\n",
    "    This is intentionally modular so you can replace encoder/attention with your own blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modalities_dims, hidden_dim=256, shared_dim=128, num_heads=4, depth_per_stage=1, stages=2, num_classes=3, dropout=0.1):\n",
    "        \"\"\"\n",
    "        modalities_dims: dict -> {\"text\": dim_text, \"audio\": dim_audio, ...}\n",
    "        hidden_dim: internal encoder hidden size\n",
    "        shared_dim: dimension to project all modalities into (embedding dim)\n",
    "        depth_per_stage: number of cross-attention blocks per pair in each stage\n",
    "        stages: number of hierarchical stages\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.modalities = list(modalities_dims.keys())\n",
    "        self.num_modalities = len(self.modalities)\n",
    "        self.shared_dim = shared_dim\n",
    "\n",
    "        # encoders per modality\n",
    "        self.encoders = nn.ModuleDict()\n",
    "        for name, d in modalities_dims.items():\n",
    "            self.encoders[name] = SimpleEncoder(d, hidden_dim, shared_dim, dropout=dropout)\n",
    "\n",
    "        # early fusion projection (projects concatenated features to shared_dim)\n",
    "        self.early_proj = nn.Sequential(\n",
    "            nn.Linear(shared_dim * self.num_modalities, shared_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LayerNorm(shared_dim)\n",
    "        )\n",
    "\n",
    "        # mid-level hierarchical cross-attention blocks.\n",
    "        # We'll create cross-attention for each pair and reuse blocks across stages if desired.\n",
    "        self.stages = stages\n",
    "        self.depth_per_stage = depth_per_stage\n",
    "\n",
    "        # build a list of cross-attention blocks for each stage\n",
    "        self.hierarchy = nn.ModuleList()\n",
    "        for s in range(stages):\n",
    "            # a small ModuleDict mapping pair->stack of cross-attention blocks\n",
    "            pair_blocks = nn.ModuleDict()\n",
    "            for i in range(self.num_modalities):\n",
    "                for j in range(i + 1, self.num_modalities):\n",
    "                    pair_name = f\"{self.modalities[i]}__{self.modalities[j]}\"\n",
    "                    # stack of blocks for this pair at this stage\n",
    "                    blocks = nn.ModuleList([CrossAttentionBlock(shared_dim, num_heads=num_heads, dropout=dropout) for _ in range(depth_per_stage)])\n",
    "                    pair_blocks[pair_name] = blocks\n",
    "            self.hierarchy.append(pair_blocks)\n",
    "\n",
    "        # fusion head (after hierarchical stages)\n",
    "        self.fusion_norm = nn.LayerNorm(shared_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(shared_dim * 2, shared_dim),  # use global pooled + early fused\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(shared_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: dict of modality_name -> tensor (batch, features)\n",
    "        returns logits (batch, num_classes)\n",
    "        \"\"\"\n",
    "        # 1) per-modality encoding\n",
    "        embeddings = {}\n",
    "        for name in self.modalities:\n",
    "            x = inputs[name]\n",
    "            emb = self.encoders[name](x)  # (batch, shared_dim)\n",
    "            embeddings[name] = emb\n",
    "\n",
    "        batch = next(iter(embeddings.values())).shape[0]\n",
    "\n",
    "        # 2) early fusion (concatenate all embeddings)\n",
    "        concat = torch.cat([embeddings[m] for m in self.modalities], dim=-1)  # (batch, shared_dim * M)\n",
    "        early = self.early_proj(concat)  # (batch, shared_dim)\n",
    "\n",
    "        # 3) hierarchical cross-attention stages\n",
    "        # We'll keep a working dict for embeddings that gets updated each stage\n",
    "        working = {k: v.unsqueeze(1) for k, v in embeddings.items()}  # (batch, seq=1, dim)\n",
    "\n",
    "        for stage_idx, pair_blocks in enumerate(self.hierarchy):\n",
    "            # iterate through modality pairs and apply their stacked blocks\n",
    "            # after processing all pairs, we can optionally aggregate or keep updated for next stage\n",
    "            updates = {}\n",
    "            # init updates with current values\n",
    "            for k in working:\n",
    "                updates[k] = working[k]\n",
    "\n",
    "            for pair_name, blocks in pair_blocks.items():\n",
    "                a_name, b_name = pair_name.split(\"__\")\n",
    "                A = updates[a_name]\n",
    "                B = updates[b_name]\n",
    "                # apply stacked blocks sequentially\n",
    "                for block in blocks:\n",
    "                    A, B = block(A, B)  # each may be (batch, seq, dim)\n",
    "                updates[a_name] = A\n",
    "                updates[b_name] = B\n",
    "\n",
    "            # optionally perform a pooling or cross-modal aggregation here\n",
    "            # for simplicity we set working=updates\n",
    "            working = updates\n",
    "\n",
    "        # squeeze sequence dim\n",
    "        final_embeddings = {k: v.squeeze(1) for k, v in working.items()}  # (batch, dim)\n",
    "\n",
    "        # 4) global pooling/aggregation and classification\n",
    "        # simple strategy: mean of modality embeddings\n",
    "        stacked = torch.stack([final_embeddings[m] for m in self.modalities], dim=1)  # (batch, M, dim)\n",
    "        global_pool = stacked.mean(dim=1)  # (batch, dim)\n",
    "\n",
    "        # combine early fusion + hierarchical global\n",
    "        combined = torch.cat([self.fusion_norm(global_pool), early], dim=-1)  # (batch, 2*dim)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # quick synthetic example\n",
    "    modalities = {\"text\": 300, \"audio\": 74, \"video\": 512}\n",
    "    model = HierarchicalFeatureFusionNetwork(modalities_dims=modalities, hidden_dim=256, shared_dim=128, stages=2, depth_per_stage=1, num_classes=5)\n",
    "\n",
    "    B = 8\n",
    "    x_text = torch.randn(B, 300)\n",
    "    x_audio = torch.randn(B, 74)\n",
    "    x_video = torch.randn(B, 512)\n",
    "\n",
    "    inputs = {\"text\": x_text, \"audio\": x_audio, \"video\": x_video}\n",
    "    logits = model(inputs)\n",
    "    print(\"logits shape:\", logits.shape)  # (B, num_classes)\n",
    "\n",
    "    # simple loss to test backward\n",
    "    labels = torch.randint(0, 5, (B,))\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    loss.backward()\n",
    "    print(\"forward+backward OK, loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
