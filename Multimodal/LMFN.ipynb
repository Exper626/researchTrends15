{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497c656-233b-49bb-b656-ddd952e9947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmfn.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------\n",
    "# Utility: sliding windows\n",
    "# -------------------------\n",
    "def sliding_windows(x, window_size, stride):\n",
    "    \"\"\"\n",
    "    x: (B, T, D)\n",
    "    returns: (B, num_windows, window_size, D)\n",
    "    \"\"\"\n",
    "    B, T, D = x.shape\n",
    "    if window_size > T:\n",
    "        # pad at end\n",
    "        pad = window_size - T\n",
    "        x = F.pad(x, (0,0,0,pad))  # pad time dimension on right\n",
    "        T = window_size\n",
    "    # compute windows\n",
    "    indices = []\n",
    "    starts = list(range(0, T - window_size + 1, stride))\n",
    "    if len(starts) == 0:\n",
    "        starts = [0]\n",
    "    windows = []\n",
    "    for s in starts:\n",
    "        windows.append(x[:, s:s+window_size, :].unsqueeze(1))  # (B,1,window_size,D)\n",
    "    return torch.cat(windows, dim=1)  # (B, num_windows, window_size, D)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Small temporal encoder (shared MLP per timestep)\n",
    "# -------------------------\n",
    "class TimeStepEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D_in) => output (B, T, hidden_dim)\n",
    "        B, T, D = x.shape\n",
    "        out = self.net(x.view(B * T, D)).view(B, T, -1)\n",
    "        return out\n",
    "\n",
    "# -------------------------\n",
    "# Local fusion block\n",
    "# -------------------------\n",
    "class LocalFusionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, window_size, fused_dim):\n",
    "        \"\"\"\n",
    "        hidden_dim: per-modality timestep representation dim\n",
    "        window_size: number of timesteps in local window\n",
    "        fused_dim: output dim of local fusion\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # gating network: takes concatenated (or pooled) per-modality window info -> gates per modality\n",
    "        # We'll implement a simple attention-like gating per modality using pooled window features.\n",
    "        self.gate_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * window_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # scalar gate per modality (we'll apply one per modality separately)\n",
    "        )\n",
    "        # local fusion MLP takes concatenated gated modality-window vectors\n",
    "        # input size: num_modalities * hidden_dim * window_size (after flatten/pooling choose)\n",
    "        # But to keep size sane, we will **pool** across time inside window (mean), so each modality contributes hidden_dim\n",
    "        self.local_fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, fused_dim),  # we'll concatenate (pooled gated + pooled ungated) as residual-aware fusion\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fused_dim, fused_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, modality_window): \n",
    "        \"\"\"\n",
    "        modality_window: (B, window_size, hidden_dim) for a single modality\n",
    "        returns: gated pooled vector (B, hidden_dim) and gate scalar (B,1)\n",
    "        \"\"\"\n",
    "        B, W, H = modality_window.shape\n",
    "        # 1) compute gate scalar per sample using flattened window\n",
    "        flat = modality_window.view(B, W * H)\n",
    "        gate_logits = self.gate_mlp(flat)  # (B,1)\n",
    "        gate = torch.sigmoid(gate_logits)  # (B,1) in 0..1\n",
    "        # 2) pooled vector (mean over window)\n",
    "        pooled = modality_window.mean(dim=1)  # (B, H)\n",
    "        gated_pooled = pooled * gate  # broadcast (B,H)\n",
    "        # 3) provide both gated pooled and pooled (residual-aware)\n",
    "        fused = self.local_fusion_mlp(torch.cat([gated_pooled, pooled], dim=1))  # (B, fused_dim)\n",
    "        return fused, gate  # (B, F), (B,1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# LMFN main model\n",
    "# -------------------------\n",
    "class LMFN(nn.Module):\n",
    "    def __init__(self, input_dims, timestep_hidden=128, window_size=5, stride=1, fused_dim=128, agg_channels=128, out_dim=2):\n",
    "        \"\"\"\n",
    "        input_dims: list of input dims for each modality (per timestep)\n",
    "        timestep_hidden: encoder output dim for each timestep per modality\n",
    "        window_size: temporal window length for local fusion\n",
    "        stride: sliding stride\n",
    "        fused_dim: local fused vector dim per window & pair (or per window)\n",
    "        agg_channels: channels for temporal aggregator (1D conv)\n",
    "        out_dim: output dimension (classes or 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_modalities = len(input_dims)\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        # timestep encoders for each modality\n",
    "        self.encoders = nn.ModuleList([TimeStepEncoder(d, timestep_hidden) for d in input_dims])\n",
    "        # local fusion blocks: one per modality (they produce fused vectors per modality-window)\n",
    "        # We'll later combine modality fused vectors by concatenation and a combiner MLP to produce single fused vector per window\n",
    "        self.local_blocks = nn.ModuleList([LocalFusionBlock(timestep_hidden, window_size, fused_dim) for _ in input_dims])\n",
    "        # combiner that merges per-modality local fused outputs into a single window-level fused vector\n",
    "        self.window_combiner = nn.Sequential(\n",
    "            nn.Linear(fused_dim * self.num_modalities, fused_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fused_dim * 2, fused_dim)\n",
    "        )\n",
    "        # temporal aggregator: 1D conv over windows (num_windows dimension)\n",
    "        self.agg_conv = nn.Conv1d(in_channels=fused_dim, out_channels=agg_channels, kernel_size=3, padding=1)\n",
    "        self.agg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(agg_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, *modal_seq):\n",
    "        \"\"\"\n",
    "        modal_seq: each tensor (B, T, D_i)\n",
    "        returns: logits (B, out_dim) and an aux dict\n",
    "        \"\"\"\n",
    "        B = modal_seq[0].shape[0]\n",
    "        # 1) encode each modality per timestep -> (B, T, H)\n",
    "        encoded = [enc(x) for enc, x in zip(self.encoders, modal_seq)]\n",
    "        # 2) build sliding windows per modality: (B, num_windows, window_size, H)\n",
    "        windows_per_mod = [sliding_windows(e, self.window_size, self.stride) for e in encoded]\n",
    "        num_windows = windows_per_mod[0].shape[1]\n",
    "        # 3) for each window index, compute local fusion across modalities\n",
    "        window_fused_list = []\n",
    "        gates = []\n",
    "        for w_idx in range(num_windows):\n",
    "            per_mod_fused = []\n",
    "            per_mod_gates = []\n",
    "            for m in range(self.num_modalities):\n",
    "                mod_win = windows_per_mod[m][:, w_idx, :, :]  # (B, window_size, H)\n",
    "                fused_vec, gate = self.local_blocks[m](mod_win)  # (B, F), (B,1)\n",
    "                per_mod_fused.append(fused_vec)\n",
    "                per_mod_gates.append(gate)\n",
    "            # concatenate per-modality fused vectors -> (B, F * num_modalities)\n",
    "            concat = torch.cat(per_mod_fused, dim=1)\n",
    "            window_fused = self.window_combiner(concat)  # (B, fused_dim)\n",
    "            window_fused_list.append(window_fused.unsqueeze(1))  # keep window dim\n",
    "            gates.append(torch.cat(per_mod_gates, dim=1).unsqueeze(1))  # (B,1,num_modalities)\n",
    "        # stack window fused: (B, num_windows, fused_dim)\n",
    "        fused_windows = torch.cat(window_fused_list, dim=1)\n",
    "        # 4) aggregate over windows using 1D conv/pooling\n",
    "        # conv expects (B, C, L) where C == fused_dim, L == num_windows\n",
    "        x = fused_windows.permute(0, 2, 1)  # (B, fused_dim, num_windows)\n",
    "        x = self.agg_conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.agg_pool(x).squeeze(-1)  # (B, agg_channels)\n",
    "        logits = self.classifier(x)  # (B, out_dim)\n",
    "        aux = {\n",
    "            \"encoded\": encoded,\n",
    "            \"fused_windows\": fused_windows,\n",
    "            \"gates\": torch.cat(gates, dim=1) if len(gates) > 0 else None  # (B, num_windows, num_modalities)\n",
    "        }\n",
    "        return logits, aux\n",
    "\n",
    "# -------------------------\n",
    "# Synthetic data & small training loop\n",
    "# -------------------------\n",
    "def synthetic_sequence_data(num_samples=400, T=30, dims=[16, 8, 12], num_classes=2):\n",
    "    torch.manual_seed(0)\n",
    "    Xs = [torch.randn(num_samples, T, d) for d in dims]\n",
    "    # make label depend on sum of mean pooled first modality over later half of sequence\n",
    "    half = T // 2\n",
    "    key_signal = Xs[0][:, half:, :].mean(dim=(1,2))\n",
    "    y = (key_signal + 0.1 * torch.randn(num_samples) > 0).long()\n",
    "    return Xs, y\n",
    "\n",
    "def train_example():\n",
    "    input_dims = [16, 8, 12]\n",
    "    model = LMFN(input_dims, timestep_hidden=64, window_size=5, stride=2, fused_dim=64, agg_channels=64, out_dim=2)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    Xs, y = synthetic_sequence_data(num_samples=600, T=30, dims=input_dims, num_classes=2)\n",
    "    dataset = TensorDataset(Xs[0], Xs[1], Xs[2], y)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(8):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for xb0, xb1, xb2, lbl in loader:\n",
    "            xb0 = xb0.to(device); xb1 = xb1.to(device); xb2 = xb2.to(device); lbl = lbl.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits, aux = model(xb0, xb1, xb2)\n",
    "            loss = loss_fn(logits, lbl)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * xb0.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == lbl).sum().item()\n",
    "            total += xb0.size(0)\n",
    "        print(f\"Epoch {epoch+1}: loss={total_loss/total:.4f} acc={correct/total:.4f}\")\n",
    "    # sample inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample = [Xs[i][:4].to(device) for i in range(len(Xs))]\n",
    "        logits, aux = model(*sample)\n",
    "        print(\"Logits sample:\", logits)\n",
    "        print(\"Fused windows shape:\", aux[\"fused_windows\"].shape)\n",
    "        print(\"Gates shape:\", aux[\"gates\"].shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
