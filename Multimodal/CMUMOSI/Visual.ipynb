{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVvFIihL4xtB",
    "outputId": "f3a58901-da25-449e-850f-144671318beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  video.zip\n",
      "   creating: video/\n",
      "  inflating: video/CMU_MOSI_Opinion_Labels.csd  \n",
      "  inflating: video/CMU_MOSI_Visual_Facet_42.csd  \n"
     ]
    }
   ],
   "source": [
    "!unzip video.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKLXyj3e4xwd",
    "outputId": "95a8d171-8577-4d60-ef36-8c3e4e22d294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "Visual top-level keys: ['FACET_4.2']\n",
      "Segment 03bSnISJMiM: shape (5402, 35)\n",
      "Segment 0h-zjBukYpk: shape (5399, 35)\n",
      "Segment 1DmNV9C1hbY: shape (2078, 35)\n",
      "Segment 1iG0909rllw: shape (4718, 35)\n",
      "Segment 2WGyTLYerpo: shape (5402, 35)\n",
      "Segment 2iD-tVS8NPw: shape (5400, 35)\n",
      "Segment 5W7Z1C_fDaE: shape (2282, 35)\n",
      "Segment 6Egk_28TtTM: shape (4744, 35)\n",
      "Segment 6_0THN4chvY: shape (3060, 35)\n",
      "Segment 73jzhE8R1TQ: shape (5402, 35)\n",
      "Segment 7JsX8y1ysxY: shape (5400, 35)\n",
      "Segment 8OtFthrtaJM: shape (5402, 35)\n",
      "Segment 8d-gEyoeBzc: shape (5400, 35)\n",
      "Segment 8qrpnFRGt2A: shape (5401, 35)\n",
      "Segment 9J25DZhivz8: shape (5400, 35)\n",
      "Segment 9T9Hf74oK10: shape (5403, 35)\n",
      "Segment 9c67fiY0wGQ: shape (2834, 35)\n",
      "Segment 9qR7uwkblbs: shape (5400, 35)\n",
      "Segment Af8D0E4ZXaw: shape (5402, 35)\n",
      "Segment BI97DNYfe5I: shape (3419, 35)\n",
      "Segment BXuRRbG0Ugk: shape (5402, 35)\n",
      "Segment Bfr499ggo-0: shape (2566, 35)\n",
      "Segment BioHAh1qJAQ: shape (5400, 35)\n",
      "Segment BvYR0L6f2Ig: shape (5400, 35)\n",
      "Segment Ci-AH39fi3Y: shape (5400, 35)\n",
      "Segment Clx4VXItLTE: shape (5400, 35)\n",
      "Segment Dg_0XKD0Mf4: shape (3613, 35)\n",
      "Segment G-xst2euQUc: shape (4162, 35)\n",
      "Segment G6GlGvlkxAQ: shape (5400, 35)\n",
      "Segment GWuJjcEuzt8: shape (4061, 35)\n",
      "Segment HEsqda8_d0Q: shape (4689, 35)\n",
      "Segment I5y0__X72p0: shape (5400, 35)\n",
      "Segment Iu2PFX3z_1s: shape (2837, 35)\n",
      "Segment IumbAb8q2dM: shape (3929, 35)\n",
      "Segment Jkswaaud0hk: shape (1764, 35)\n",
      "Segment LSi-o-IrDMs: shape (5400, 35)\n",
      "Segment MLal-t_vJPM: shape (3300, 35)\n",
      "Segment Njd1F0vZSm4: shape (2524, 35)\n",
      "Segment Nzq88NnDkEk: shape (4991, 35)\n",
      "Segment OQvJTdtJ2H4: shape (3046, 35)\n",
      "Segment OtBXNcAL_lE: shape (5400, 35)\n",
      "Segment Oz06ZWiO20M: shape (5399, 35)\n",
      "Segment POKffnXeBds: shape (2789, 35)\n",
      "Segment PZ-lDQFboO8: shape (3056, 35)\n",
      "Segment QN9ZIUWUXsY: shape (5401, 35)\n",
      "Segment Qr1Ca94K55A: shape (4218, 35)\n",
      "Segment Sqr0AcuoNnk: shape (5400, 35)\n",
      "Segment TvyZBvOMOTc: shape (2158, 35)\n",
      "Segment VCslbP0mgZI: shape (5400, 35)\n",
      "Segment VbQk4H8hgr0: shape (5399, 35)\n",
      "Segment Vj1wYRQjB-o: shape (5399, 35)\n",
      "Segment W8NXH0Djyww: shape (5330, 35)\n",
      "Segment WKA5OygbEKI: shape (5183, 35)\n",
      "Segment X3j2zQgwYgE: shape (2304, 35)\n",
      "Segment ZAIRrfG22O0: shape (1645, 35)\n",
      "Segment ZUXBRvtny7o: shape (5400, 35)\n",
      "Segment _dI--eQ6qVU: shape (5400, 35)\n",
      "Segment aiEXnCPZubE: shape (5402, 35)\n",
      "Segment atnd_PF-Lbs: shape (5401, 35)\n",
      "Segment bOL9jKpeJRs: shape (3626, 35)\n",
      "Segment bvLlb-M3UXU: shape (5400, 35)\n",
      "Segment c7UH_rxdZv4: shape (5400, 35)\n",
      "Segment cM3Yna7AavY: shape (5400, 35)\n",
      "Segment cW1FSBF59ik: shape (5402, 35)\n",
      "Segment cXypl4FnoZo: shape (5402, 35)\n",
      "Segment d3_k5Xpfmik: shape (5400, 35)\n",
      "Segment d6hH302o4v8: shape (4398, 35)\n",
      "Segment dq3Nf_lMPnE: shape (2993, 35)\n",
      "Segment etzxEpPuc6I: shape (5400, 35)\n",
      "Segment f9O3YtZ2VfI: shape (4577, 35)\n",
      "Segment f_pcplsH_V0: shape (5400, 35)\n",
      "Segment fvVhgmXxadc: shape (5402, 35)\n",
      "Segment iiK8YX8oH1E: shape (2813, 35)\n",
      "Segment jUzDDGyPkXU: shape (5403, 35)\n",
      "Segment k5Y_838nuGo: shape (5400, 35)\n",
      "Segment lXPQBPVc5Cw: shape (5400, 35)\n",
      "Segment nbWiPyCm4g0: shape (5400, 35)\n",
      "Segment nzpVDcQ0ywM: shape (5400, 35)\n",
      "Segment ob23OKe5a9Q: shape (4282, 35)\n",
      "Segment pLTX3ipuDJI: shape (3590, 35)\n",
      "Segment phBUpBr1hSo: shape (5400, 35)\n",
      "Segment rnaNMUZpvvg: shape (5400, 35)\n",
      "Segment tIrG4oNLFzE: shape (5400, 35)\n",
      "Segment tStelxIAHjw: shape (3938, 35)\n",
      "Segment tmZoasNr4rU: shape (5400, 35)\n",
      "Segment v0zCBqDeKcE: shape (5403, 35)\n",
      "Segment vvZ4IcEtiZc: shape (5400, 35)\n",
      "Segment vyB00TXsimI: shape (5403, 35)\n",
      "Segment wMbj6ajWbic: shape (5400, 35)\n",
      "Segment yDtzw_Y-7RU: shape (5400, 35)\n",
      "Segment yvsjCA6Y5Fc: shape (5402, 35)\n",
      "Segment zhpQhgha_KU: shape (5400, 35)\n",
      "Generated 4179 visual chunks from 92 segments\n",
      "Final visual dataset: 4179 chunks, labels [-3.00, 3.00]\n",
      "----------------------------------------------------------------------\n",
      "Starting vision-only training: 3343 train / 836 val chunks\n",
      "\n",
      "Epoch 01 | Train 2.6240 | Val 2.3955\n",
      "  → New best! Val MSE = 2.3955\n",
      "Epoch 02 | Train 2.2924 | Val 1.9095\n",
      "  → New best! Val MSE = 1.9095\n",
      "Epoch 03 | Train 1.8124 | Val 1.4728\n",
      "  → New best! Val MSE = 1.4728\n",
      "Epoch 04 | Train 1.5389 | Val 1.3250\n",
      "  → New best! Val MSE = 1.3250\n",
      "Epoch 05 | Train 1.3064 | Val 1.2520\n",
      "  → New best! Val MSE = 1.2520\n",
      "Epoch 06 | Train 1.1412 | Val 1.0634\n",
      "  → New best! Val MSE = 1.0634\n",
      "Epoch 07 | Train 0.9951 | Val 0.9073\n",
      "  → New best! Val MSE = 0.9073\n",
      "Epoch 08 | Train 0.8733 | Val 0.8442\n",
      "  → New best! Val MSE = 0.8442\n",
      "Epoch 09 | Train 0.7237 | Val 0.6888\n",
      "  → New best! Val MSE = 0.6888\n",
      "Epoch 10 | Train 0.6725 | Val 0.6622\n",
      "  → New best! Val MSE = 0.6622\n",
      "Epoch 11 | Train 0.5813 | Val 0.6466\n",
      "  → New best! Val MSE = 0.6466\n",
      "Epoch 12 | Train 0.5929 | Val 0.4824\n",
      "  → New best! Val MSE = 0.4824\n",
      "Epoch 13 | Train 0.4905 | Val 0.5200\n",
      "Epoch 14 | Train 0.4832 | Val 0.6019\n",
      "Epoch 15 | Train 0.4069 | Val 0.4180\n",
      "  → New best! Val MSE = 0.4180\n",
      "Epoch 16 | Train 0.3843 | Val 0.3769\n",
      "  → New best! Val MSE = 0.3769\n",
      "Epoch 17 | Train 0.3648 | Val 0.3729\n",
      "  → New best! Val MSE = 0.3729\n",
      "Epoch 18 | Train 0.3434 | Val 0.3414\n",
      "  → New best! Val MSE = 0.3414\n",
      "Epoch 19 | Train 0.3115 | Val 0.3185\n",
      "  → New best! Val MSE = 0.3185\n",
      "Epoch 20 | Train 0.2891 | Val 0.2863\n",
      "  → New best! Val MSE = 0.2863\n",
      "Epoch 21 | Train 0.2808 | Val 0.3024\n",
      "Epoch 22 | Train 0.2770 | Val 0.3380\n",
      "Epoch 23 | Train 0.2624 | Val 0.3050\n",
      "Epoch 24 | Train 0.2498 | Val 0.2890\n",
      "Epoch 25 | Train 0.2219 | Val 0.3321\n",
      "Epoch 26 | Train 0.2323 | Val 0.3170\n",
      "Epoch 27 | Train 0.1927 | Val 0.2695\n",
      "  → New best! Val MSE = 0.2695\n",
      "Epoch 28 | Train 0.1693 | Val 0.2284\n",
      "  → New best! Val MSE = 0.2284\n",
      "Epoch 29 | Train 0.1539 | Val 0.1950\n",
      "  → New best! Val MSE = 0.1950\n",
      "Epoch 30 | Train 0.1519 | Val 0.1934\n",
      "  → New best! Val MSE = 0.1934\n",
      "Epoch 31 | Train 0.1542 | Val 0.1901\n",
      "  → New best! Val MSE = 0.1901\n",
      "Epoch 32 | Train 0.1588 | Val 0.1960\n",
      "Epoch 33 | Train 0.1436 | Val 0.1584\n",
      "  → New best! Val MSE = 0.1584\n",
      "Epoch 34 | Train 0.1477 | Val 0.2047\n",
      "Epoch 35 | Train 0.1317 | Val 0.1569\n",
      "  → New best! Val MSE = 0.1569\n",
      "Epoch 36 | Train 0.1344 | Val 0.1262\n",
      "  → New best! Val MSE = 0.1262\n",
      "Epoch 37 | Train 0.1312 | Val 0.1813\n",
      "Epoch 38 | Train 0.1290 | Val 0.1794\n",
      "Epoch 39 | Train 0.1219 | Val 0.1479\n",
      "Epoch 40 | Train 0.1222 | Val 0.1404\n",
      "Epoch 41 | Train 0.1149 | Val 0.1384\n",
      "Epoch 42 | Train 0.1169 | Val 0.1612\n",
      "Epoch 43 | Train 0.1083 | Val 0.1172\n",
      "  → New best! Val MSE = 0.1172\n",
      "Epoch 44 | Train 0.1004 | Val 0.1152\n",
      "  → New best! Val MSE = 0.1152\n",
      "Epoch 45 | Train 0.1082 | Val 0.1163\n",
      "Epoch 46 | Train 0.0987 | Val 0.1087\n",
      "  → New best! Val MSE = 0.1087\n",
      "Epoch 47 | Train 0.0976 | Val 0.1075\n",
      "  → New best! Val MSE = 0.1075\n",
      "Epoch 48 | Train 0.0960 | Val 0.1046\n",
      "  → New best! Val MSE = 0.1046\n",
      "Epoch 49 | Train 0.0987 | Val 0.1098\n",
      "Epoch 50 | Train 0.0954 | Val 0.1058\n",
      "Epoch 51 | Train 0.0937 | Val 0.1182\n",
      "Epoch 52 | Train 0.0948 | Val 0.1191\n",
      "Epoch 53 | Train 0.0957 | Val 0.1005\n",
      "  → New best! Val MSE = 0.1005\n",
      "Epoch 54 | Train 0.0927 | Val 0.0939\n",
      "  → New best! Val MSE = 0.0939\n",
      "Epoch 55 | Train 0.0942 | Val 0.0990\n",
      "Epoch 56 | Train 0.0862 | Val 0.1114\n",
      "Epoch 57 | Train 0.0890 | Val 0.1052\n",
      "Epoch 58 | Train 0.0910 | Val 0.0884\n",
      "  → New best! Val MSE = 0.0884\n",
      "Epoch 59 | Train 0.0891 | Val 0.0916\n",
      "Epoch 60 | Train 0.0857 | Val 0.1021\n",
      "Epoch 61 | Train 0.0916 | Val 0.0978\n",
      "Epoch 62 | Train 0.0860 | Val 0.0913\n",
      "Epoch 63 | Train 0.0921 | Val 0.1040\n",
      "Epoch 64 | Train 0.0887 | Val 0.1397\n",
      "Epoch 65 | Train 0.0838 | Val 0.1290\n",
      "Epoch 66 | Train 0.0811 | Val 0.1280\n",
      "Epoch 67 | Train 0.0810 | Val 0.1053\n",
      "Epoch 68 | Train 0.0859 | Val 0.1084\n",
      "Epoch 69 | Train 0.0783 | Val 0.1074\n",
      "Epoch 70 | Train 0.0795 | Val 0.1356\n",
      "Epoch 71 | Train 0.0768 | Val 0.1070\n",
      "Epoch 72 | Train 0.0780 | Val 0.0992\n",
      "Epoch 73 | Train 0.0801 | Val 0.0924\n",
      "Early stopping\n",
      "\n",
      "Vision-only training finished! Best Val MSE: 0.0884\n",
      "Model saved as 'best_mosi_visual_facet42.pth'\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ============================= GPU SETUP =============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# =========================== LOAD + CHUNK VISUAL FEATURES (FINAL FIXED) ===========================\n",
    "def load_and_chunk_visual(visual_path, label_path, chunk_len=200, stride=100):\n",
    "    with h5py.File(visual_path, 'r') as f:\n",
    "        print(\"Visual top-level keys:\", list(f.keys()))\n",
    "        data_group = f['FACET_4.2']['data']\n",
    "        segment_ids = sorted(data_group.keys())\n",
    "\n",
    "        chunks = []\n",
    "        seg_lengths = {}\n",
    "\n",
    "        for seg_id in segment_ids:\n",
    "            feats = data_group[seg_id]['features'][:]\n",
    "            feats = np.nan_to_num(feats, nan=0.0).astype(np.float32)\n",
    "            print(f\"Segment {seg_id}: shape {feats.shape}\")  # This will show (T, 35)\n",
    "            L = len(feats)\n",
    "            seg_lengths[seg_id] = L\n",
    "            for i in range(0, max(1, L - chunk_len + 1), stride):\n",
    "                chunks.append(feats[i:i+chunk_len])\n",
    "\n",
    "    print(f\"Generated {len(chunks)} visual chunks from {len(segment_ids)} segments\")\n",
    "\n",
    "    # Load labels\n",
    "    with h5py.File(label_path, 'r') as f:\n",
    "        label_group = f['Opinion Segment Labels']['data']\n",
    "        labels = []\n",
    "        for seg_id in segment_ids:\n",
    "            raw = label_group[seg_id]['features'][:]\n",
    "            if raw.shape[-1] == 7:\n",
    "                label = float(np.dot(raw.flatten(), np.arange(-3, 4)))\n",
    "            else:\n",
    "                label = float(raw.flatten()[0])\n",
    "            n_chunks = max(1, (seg_lengths[seg_id] - chunk_len + stride) // stride)\n",
    "            labels.extend([label] * n_chunks)\n",
    "\n",
    "    chunks = chunks[:len(labels)]\n",
    "    assert len(chunks) == len(labels)\n",
    "    print(f\"Final visual dataset: {len(chunks)} chunks, labels [{min(labels):.2f}, {max(labels):.2f}]\")\n",
    "    print(\"-\" * 70)\n",
    "    return chunks, labels\n",
    "\n",
    "\n",
    "# =========================== DATASET (35-dim FACET 4.2) ===========================\n",
    "class VisualDataset(Dataset):\n",
    "    def __len__(self): return len(self.chunks)\n",
    "    def __init__(self, chunks, labels):\n",
    "        self.chunks = chunks\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.chunks[i]), torch.tensor(self.labels[i], dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labs = zip(*batch)\n",
    "    lengths = torch.tensor([s.shape[0] for s in seqs])\n",
    "    max_len = lengths.max().item()\n",
    "    batch_size = len(seqs)\n",
    "    feature_dim = seqs[0].shape[1]               # ← Auto-detects 35\n",
    "    padded = torch.zeros(batch_size, max_len, feature_dim, dtype=torch.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        padded[i, :lengths[i]] = s\n",
    "    labels = torch.stack(labs).unsqueeze(1)\n",
    "    return padded, labels, lengths\n",
    "\n",
    "\n",
    "# =========================== MODEL (35 → 128 → bi-LSTM) ===========================\n",
    "class VisualSentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=35):   # ← Correct input size\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, num_layers=2, batch_first=True,\n",
    "                            bidirectional=True, dropout=0.4)\n",
    "        self.norm = nn.LayerNorm(256)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        h = torch.cat((hn[-2], hn[-1]), dim=1)\n",
    "        h = self.norm(h)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "# =========================== TRAINING ===========================\n",
    "def train_visual():\n",
    "    chunks, labels = load_and_chunk_visual(\n",
    "        'video/CMU_MOSI_Visual_Facet_42.csd',\n",
    "        'video/CMU_MOSI_Opinion_Labels.csd',\n",
    "        chunk_len=200, stride=100\n",
    "    )\n",
    "\n",
    "    train_c, val_c, train_y, val_y = train_test_split(\n",
    "        chunks, labels, test_size=0.2, random_state=42,\n",
    "        stratify=[int(l > 0) for l in labels]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(VisualDataset(train_c, train_y), batch_size=64,\n",
    "                              shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "    val_loader   = DataLoader(VisualDataset(val_c, val_y), batch_size=64,\n",
    "                              shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = VisualSentimentLSTM(input_dim=35).to(device)  # ← 35-dim\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val = float('inf')\n",
    "    patience = 15\n",
    "    no_improve = 0\n",
    "\n",
    "    print(f\"Starting vision-only training: {len(train_c)} train / {len(val_c)} val chunks\\n\")\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y, lengths in train_loader:\n",
    "            x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x, lengths)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y, lengths in val_loader:\n",
    "                x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
    "                pred = model(x, lengths)\n",
    "                val_loss += criterion(pred, y).item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss   /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train {train_loss:.4f} | Val {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), \"best_mosi_visual_facet42.pth\")\n",
    "            print(f\"  → New best! Val MSE = {best_val:.4f}\")\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nVision-only training finished! Best Val MSE: {best_val:.4f}\")\n",
    "    print(\"Model saved as 'best_mosi_visual_facet42.pth'\")\n",
    "\n",
    "\n",
    "# =========================== RUN ===========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_visual()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
