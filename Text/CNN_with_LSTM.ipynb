{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjfIHy7R6LeE"
   },
   "source": [
    "# **IMDB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOi161KmoA9b"
   },
   "source": [
    "# **CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-glkGJHtAPwK",
    "outputId": "eca633ad-cb15-4777-e5ca-7327f87a3b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiBranchCNN_LSTM(\n",
      "  (embedding): Embedding(5000, 32)\n",
      "  (branches): ModuleList(\n",
      "    (0): ModuleDict(\n",
      "      (conv): Conv1d(32, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lstm): LSTM(128, 128, batch_first=True)\n",
      "    )\n",
      "    (1): ModuleDict(\n",
      "      (conv): Conv1d(32, 128, kernel_size=(5,), stride=(1,), padding=same)\n",
      "      (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lstm): LSTM(128, 128, batch_first=True)\n",
      "    )\n",
      "    (2): ModuleDict(\n",
      "      (conv): Conv1d(32, 128, kernel_size=(7,), stride=(1,), padding=same)\n",
      "      (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lstm): LSTM(128, 128, batch_first=True)\n",
      "    )\n",
      "    (3): ModuleDict(\n",
      "      (conv): Conv1d(32, 128, kernel_size=(9,), stride=(1,), padding=same)\n",
      "      (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (lstm): LSTM(128, 128, batch_first=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parameters from your paper\n",
    "vocab_size = 5000\n",
    "embedding_dim = 32\n",
    "max_sequence_length = 500\n",
    "kernel_sizes = [3, 5, 7, 9]  # Multiple branches\n",
    "num_filters = 128\n",
    "pool_size = 2\n",
    "lstm_units = 128\n",
    "dropout_rate = 0.5\n",
    "\n",
    "class MultiBranchCNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiBranchCNN_LSTM, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Create branches for each kernel size\n",
    "        self.branches = nn.ModuleList()\n",
    "        for k in kernel_sizes:\n",
    "            branch = nn.ModuleDict({\n",
    "                'conv': nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k, padding='same'),\n",
    "                'pool': nn.MaxPool1d(kernel_size=pool_size),\n",
    "                'dropout': nn.Dropout(dropout_rate),\n",
    "                'batch_norm': nn.BatchNorm1d(num_filters),\n",
    "                'lstm': nn.LSTM(input_size=num_filters, hidden_size=lstm_units, batch_first=True)\n",
    "            })\n",
    "            self.branches.append(branch)\n",
    "\n",
    "        # Final dense layer\n",
    "        self.fc = nn.Linear(lstm_units * len(kernel_sizes), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x = self.embedding(x)                   # [batch_size, seq_len, embedding_dim]\n",
    "        x = x.permute(0, 2, 1)                 # [batch_size, embedding_dim, seq_len] for Conv1d\n",
    "\n",
    "        branch_outputs = []\n",
    "        for branch in self.branches:\n",
    "            out = F.relu(branch['conv'](x))      # Conv + ReLU\n",
    "            out = branch['pool'](out)            # MaxPooling\n",
    "            out = branch['dropout'](out)         # Dropout\n",
    "            out = branch['batch_norm'](out)      # BatchNorm\n",
    "\n",
    "            out = out.permute(0, 2, 1)           # [batch_size, seq_len//pool, num_filters] for LSTM\n",
    "            out, (h_n, c_n) = branch['lstm'](out)\n",
    "            out = out[:, -1, :]                  # Take last timestep\n",
    "            branch_outputs.append(out)\n",
    "\n",
    "        # Concatenate all branch outputs\n",
    "        out = torch.cat(branch_outputs, dim=1)   # [batch_size, lstm_units * num_branches]\n",
    "        out = torch.sigmoid(self.fc(out))        # Final output\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "model = MultiBranchCNN_LSTM()\n",
    "print(model)\n",
    "\n",
    "# Example input\n",
    "batch_size = 32\n",
    "dummy_input = torch.randint(0, vocab_size, (batch_size, max_sequence_length))\n",
    "output = model(dummy_input)\n",
    "print(output.shape)  # Should be [32, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VA_T-ZVye27g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504,
     "referenced_widgets": [
      "124d71944aa24e6e981d927b3c660184",
      "a799a50ff4214e11b004c76a8949db10",
      "0c972a4f150c49ffb5e6d5029f9e5ea7",
      "5009d62e7d4d458892bc0ad254f8bf12",
      "02b21e9ce10742968b81eb80cdf318f7",
      "5bfc64ee72a1455fa30b54e5486114bf",
      "91221f19ef9f4c66a154bcbe5a37b51a",
      "6f319c4819524ee488227af4f49da307",
      "c4dca13f6f9c4a1f93b6b649a17c1be0",
      "e7f64df896264803b44ed8d8fb917496",
      "2641bc491afd472091311573bde86702",
      "5124429118834a6e90bb555cb66113d4",
      "098d7f312fc84a3097742e3254b81a0a",
      "1c6e077bed5246318f53379a2eab6864",
      "3511737b689b4126b6e4ee9e21058286",
      "76f2255242b44db19753fcb5140a5344",
      "b4ade50347d843b7a6baa9de0902b507",
      "1e036d5c1666423281d541786b72a7c7",
      "f61809853ca04fb991cbe204872b10eb",
      "8e2634cf249f44e48a18e9869cd99a91",
      "0f779ba3b6c5492db862de0e13e2820d",
      "7ca1cfe64a5444afba3e16ee587c8a10",
      "7f7e58645f704afcb1c9160d0f418ec0",
      "563251a515d248229ec628c798fdca43",
      "dd9749422d194900b33da5d37d4a4228",
      "6c74b64a119d465a8a119b16567f8eb0",
      "2ebda8af88784466a3f929dabc424165",
      "a2ba43d1f4c244fea41464ef8980a846",
      "d952b834af21423ca7c10623b5c29232",
      "d23f7a613fab432c98dd9032e96168cd",
      "f0da05a725fd497ca6923fdd5166e0ae",
      "6f60adf930be4e9099968e20cad9d6d7",
      "2a306e52e18345b39c68612a83590baa",
      "378503d52572429a834d5a972d61d674",
      "40f597b6457744249ba5fe1f5b80255b",
      "261353cc14204fb9af4f26040337f597",
      "b4f1f2327d46435384cf2119e77e8083",
      "d0151946507e4ef08104a474b62452ab",
      "7701f13ca5604d56ad42057db13f16be",
      "b6503b4719b5487fb360a4e16e12bbac",
      "be8cb8f2f9344621990af4914a13fcba",
      "d4f2bcad69784b4daf544f120777758d",
      "b9d6064845cb42dea5660b9edb1b858d",
      "cbe124f660c1493092adfe5afafe9f72",
      "7ee47a15d21c44c3b49eb435ad699088",
      "cd34bd89b2cb4db5a1c69152d6fbb1ff",
      "5b37c80be60f4ec999c8332819ddaf61",
      "3650d850a6854de3a1f08b6aa56a2fe1",
      "380e409144744979b6470623e98b528d",
      "0f6a49e9dd234ffb95008d9b6922f00e",
      "76d777689ea54ecb9c7a6c8f818dd145",
      "1b8a3b26386c4050ad60961f49c3d6ca",
      "c21fb29005ae4ea2a3fa49ec9192e81d",
      "673cea7f33494cfda7f5d3f23a069d64",
      "993dc78094494cb19e53cf282dfd937f",
      "6eedbc9dddad4512b38be0855d187149",
      "ff2237f83ef247b580ff4327a101bf58",
      "70e1419202a046a28120187c83360fde",
      "e3a2c153073a41b393c898695855fc92",
      "6d344a9b31944a8b9e6f5cc7dd70a38f",
      "fbbe098a5f014d93bf137e30fbf79de3",
      "fee8880d0d414e97a9eeeb6c09b4f4de",
      "8828d2b444bb4d419c31bd270c3e3b2e",
      "00c8b7081ba346b2ab371f9399dd9420",
      "be746aef2510495e83172a6e6d0d9ada",
      "8d5bf49be55346e089e8d1f360ee2e41",
      "c71323af3547443290a2e0ba6812ad02",
      "4a8cee0bba4c40c4b353d140d7ddd572",
      "6b44666ef976419da57291cb5795c9ac",
      "3576750b1a1647b0a8aaef5a26339649",
      "44af03e26d574969a94c2d5a108baff9",
      "d522fe41b66f4a15b4c86c88c4168db2",
      "c4377fd5f3b84a0590be518345e7dd8f",
      "299506a0ea62475d9fc845422dbbc07b",
      "a2b0c21da28c4f3b94ce74b54390c0df",
      "6504e6cc50c8465089efc792ed582c6e",
      "9744f2da41cf42f3b8e7e7ebb45e9d83"
     ]
    },
    "id": "bW5dOY6mfxl9",
    "outputId": "bee90d85-9f8e-4e86-b682-3665b5993570"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124d71944aa24e6e981d927b3c660184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5124429118834a6e90bb555cb66113d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7e58645f704afcb1c9160d0f418ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378503d52572429a834d5a972d61d674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee47a15d21c44c3b49eb435ad699088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eedbc9dddad4512b38be0855d187149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71323af3547443290a2e0ba6812ad02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss = 0.6958\n",
      "Epoch 2/2, Loss = 0.6919\n",
      "\n",
      "Test Accuracy: 0.5060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# Load IMDB Dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Tokenization + Vocabulary Building\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Build vocab from training set\n",
    "word_counts = Counter()\n",
    "for item in train_data:\n",
    "    tokens = tokenize(item[\"text\"])\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "vocab_size = 5000  # keep top 5000 words\n",
    "most_common = word_counts.most_common(vocab_size - 2)\n",
    "\n",
    "# Reserve special tokens\n",
    "word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (word, _) in enumerate(most_common, start=2):\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "idx_to_word = {idx: w for w, idx in word_to_idx.items()}\n",
    "\n",
    "# Text → Integer Sequence Conversion\n",
    "max_len = 500\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    sequence = [word_to_idx.get(token, 1) for token in tokens]  # 1 = <UNK>\n",
    "\n",
    "    if len(sequence) > max_len:\n",
    "        sequence = sequence[:max_len]\n",
    "    else:\n",
    "        sequence = sequence + [0] * (max_len - len(sequence))  # pad with <PAD>\n",
    "\n",
    "    return sequence\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, imdb_split):\n",
    "        self.texts = [encode_text(item[\"text\"]) for item in imdb_split]\n",
    "        self.labels = [item[\"label\"] for item in imdb_split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "test_dataset = IMDBDataset(test_data)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Multi-Branch CNN + LSTM Model\n",
    "embedding_dim = 32\n",
    "kernel_sizes = [3, 5, 7, 9]\n",
    "num_filters = 128\n",
    "pool_size = 2\n",
    "lstm_units = 128\n",
    "dropout_rate = 0.5\n",
    "\n",
    "class MultiBranchCNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.branches = nn.ModuleList()\n",
    "        for k in kernel_sizes:\n",
    "            branch = nn.ModuleDict({\n",
    "                'conv': nn.Conv1d(embedding_dim, num_filters, kernel_size=k, padding='same'),\n",
    "                'batch_norm': nn.BatchNorm1d(num_filters),\n",
    "                'dropout': nn.Dropout(dropout_rate),\n",
    "                'pool': nn.MaxPool1d(pool_size),\n",
    "                'lstm': nn.LSTM(num_filters, lstm_units, batch_first=True)\n",
    "            })\n",
    "            self.branches.append(branch)\n",
    "\n",
    "        self.fc = nn.Linear(lstm_units * len(kernel_sizes), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)              # [B, L, E]\n",
    "        x = x.permute(0, 2, 1)            # [B, E, L]\n",
    "\n",
    "        branch_outputs = []\n",
    "        for branch in self.branches:\n",
    "            out = F.relu(branch['conv'](x))\n",
    "            out = branch['batch_norm'](out)\n",
    "            out = branch['pool'](out)\n",
    "            out = branch['dropout'](out)\n",
    "\n",
    "            # For LSTM: [B, L', C]\n",
    "            out = out.permute(0, 2, 1)\n",
    "            out, (h, c) = branch['lstm'](out)\n",
    "            out = out[:, -1, :]           # last timestep\n",
    "\n",
    "            branch_outputs.append(out)\n",
    "\n",
    "        combined = torch.cat(branch_outputs, dim=1)\n",
    "        return torch.sigmoid(self.fc(combined))\n",
    "\n",
    "model = MultiBranchCNN_LSTM()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Training Setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 2\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_batch).squeeze()\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss = {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        preds = model(x_batch).squeeze()\n",
    "        preds = (preds >= 0.5).float()\n",
    "\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tMM71BPfyNP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
